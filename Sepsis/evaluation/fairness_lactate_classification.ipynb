{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import pickle\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interp\n",
    "\n",
    "import seaborn as sns\n",
    "from seaborn import heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, auc \n",
    "from sklearn.metrics import roc_curve, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, average_precision_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from netcal.scaling import TemperatureScaling\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "from shap import summary_plot\n",
    "\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_real = pd.read_csv('data/all_data.csv')\n",
    "\n",
    "all_data_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = pd.read_csv('data/generation_0/fake_data.csv')\n",
    "\n",
    "fake_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data['ethnicity'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data_real.shape)\n",
    "\n",
    "print(fake_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data.columns = all_data_real.columns\n",
    "fake_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SpO2 numeric\n",
    "fake_data = fake_data.drop('SpO2', axis=1)\n",
    "fake_data = fake_data.rename(columns={'SpO2.1': 'SpO2'})\n",
    "\n",
    "\n",
    "all_data_real = all_data_real.drop('SpO2', axis=1)\n",
    "all_data_real = all_data_real.rename(columns={'SpO2.1': 'SpO2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data_real.shape)\n",
    "\n",
    "print(fake_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for both dataframes\n",
    "fake_data_summary = fake_data.describe()\n",
    "all_data_real_summary = all_data_real.describe()\n",
    "\n",
    "# Compare means and standard deviations\n",
    "comparison = pd.DataFrame({\n",
    "    'fake_mean': fake_data_summary.loc['mean'],\n",
    "    'real_mean': all_data_real_summary.loc['mean'],\n",
    "    'fake_std': fake_data_summary.loc['std'],\n",
    "    'real_std': all_data_real_summary.loc['std']\n",
    "})\n",
    "\n",
    "# Calculate differences\n",
    "comparison['mean_diff'] = abs(comparison['fake_mean'] - comparison['real_mean'])\n",
    "comparison['std_diff'] = abs(comparison['fake_std'] - comparison['real_std'])\n",
    "\n",
    "# Display the comparison\n",
    "print(comparison)\n",
    "\n",
    "# Optionally, you can set a threshold for significant differences\n",
    "threshold = 0.1  # Adjust this value as needed\n",
    "significant_diff = comparison[(comparison['mean_diff'] > threshold) | (comparison['std_diff'] > threshold)]\n",
    "print(\"\\nVariables with significant differences:\")\n",
    "print(significant_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets\n",
    "fake_data['data_type'] = 'fake'\n",
    "all_data_real['data_type'] = 'real'\n",
    "\n",
    "merged_data = pd.concat([all_data_real, fake_data], ignore_index=True)\n",
    "\n",
    "print(merged_data['data_type'].value_counts())\n",
    "print(merged_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the ethnicities\n",
    "ethnicity_counts = merged_data['ethnicity'].value_counts()\n",
    "\n",
    "print(\"Ethnicity counts:\")\n",
    "print(ethnicity_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add patient IDs and timestamps (bloc)\n",
    "merged_data['patient_id'] = merged_data.index // 15\n",
    "\n",
    "merged_data['bloc'] = merged_data.groupby('patient_id').cumcount() + 1\n",
    "\n",
    "# Reorder columns to put 'patient_id' and 'bloc' first\n",
    "cols = merged_data.columns.tolist()\n",
    "cols = ['patient_id', 'bloc'] + [col for col in cols if col not in ['patient_id', 'bloc']]\n",
    "merged_data = merged_data[cols]\n",
    "\n",
    "merged_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix 'age' as a static variable for fake data\n",
    "mean_age = merged_data.groupby('patient_id')['age'].transform('mean')\n",
    "\n",
    "merged_data['age'] = mean_age\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change categorical variables to numeric with mean of the corresponding interval\n",
    "\n",
    "variable_dict = torch.load(\"data/A001_BTS_nonFloat\")\n",
    "\n",
    "def get_interval_mean(value, quantiles):\n",
    "    if not quantiles:  # For binary variables or those without quantiles\n",
    "        return value\n",
    "    for i in range(len(quantiles) - 1):\n",
    "        if value <= i:  # Map the integer encoding to the correct interval\n",
    "            return (quantiles[i] + quantiles[i+1]) / 2\n",
    "    return (quantiles[-2] + quantiles[-1]) / 2  # For values in the last interval\n",
    "\n",
    "for name, type_, quantiles in zip(variable_dict['Name'], variable_dict['Type'], variable_dict['Quantiles']):\n",
    "    if name in merged_data.columns:\n",
    "        if type_ in ['bin', 'GCS']:  # Binary or GCS, no transformation needed\n",
    "            continue\n",
    "        elif type_ in ['Temp_C', 'cat']:\n",
    "            merged_data[name] = merged_data[name].apply(lambda x: get_interval_mean(x, quantiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['label'] = np.nan\n",
    "\n",
    "merged_data.loc[(merged_data.Arterial_lactate > 4), 'label'] = 1\n",
    "\n",
    "merged_data.loc[merged_data.label != 1, 'label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count 'lactate' > 4 for each timestep\n",
    "filtered_df = merged_data[merged_data['label'] == 1]\n",
    "\n",
    "bloc_counts = filtered_df.groupby('bloc').size()\n",
    "\n",
    "bloc_counts_df = bloc_counts.reset_index(name='count')\n",
    "\n",
    "print(bloc_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame where 'bloc' is 15\n",
    "bloc_15_df = merged_data[merged_data['bloc'] == 15]\n",
    "\n",
    "# Determine which patients have a 'lactate' value greater than 4 in their last timestep\n",
    "patients_with_high_lactate = bloc_15_df[bloc_15_df['Arterial_lactate'] > 4]['patient_id']\n",
    "\n",
    "# Create a new label column in the original DataFrame\n",
    "merged_data['label'] = merged_data['patient_id'].apply(lambda x: 1 if x in patients_with_high_lactate.values else 0)\n",
    "\n",
    "print(merged_data['label'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count positive labels for each ethnicity\n",
    "filtered_df = merged_data[merged_data['label'] == 1]\n",
    "\n",
    "ethnicity_counts = filtered_df.groupby('ethnicity')['patient_id'].nunique().reset_index(name='count')\n",
    "\n",
    "print(ethnicity_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the last timesteps\n",
    "merged_data = merged_data[merged_data['bloc'] != 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any NaN in any column\n",
    "\n",
    "has_nan = merged_data.isna().any().any()  \n",
    "\n",
    "if has_nan:\n",
    "    print(\"There are NaN values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"No NaN values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = [\n",
    "    'age', 'HR', 'SysBP', 'MeanBP', 'DiaBP', 'RR', 'Potassium', 'Sodium', 'Chloride', 'Calcium',\n",
    "    'Ionised_Ca', 'CO2_mEqL', 'Albumin', 'Hb', 'Arterial_pH', 'Arterial_BE', 'HCO3', 'FiO2_1',\n",
    "    'Glucose', 'BUN', 'Creatinine', 'Magnesium', 'SGOT', 'SGPT', 'Total_bili', 'WBC_count',\n",
    "    'Platelets_count', 'paO2', 'paCO2', 'Arterial_lactate', 'input_total', 'input_4hourly',\n",
    "    'max_dose_vaso', 'output_total', 'output_4hourly', \n",
    "    'gender', 're_admission', 'mechvent',\n",
    "    'Temp_C', \n",
    "    'GCS', \n",
    "    'SpO2', 'PTT', 'PT', 'INR'\n",
    "]\n",
    "\n",
    "norm = [\n",
    "    'age', 'HR', 'SysBP', 'MeanBP', 'DiaBP', 'RR', 'Potassium', 'Sodium', 'Chloride', 'Calcium',\n",
    "    'Ionised_Ca', 'CO2_mEqL', 'Albumin', 'Hb', 'Arterial_pH', 'Arterial_BE', 'HCO3', 'FiO2_1',\n",
    "    'Glucose', 'BUN', 'Creatinine', 'Magnesium', 'SGOT', 'SGPT', 'Total_bili', 'WBC_count',\n",
    "    'Platelets_count', 'paO2', 'paCO2', 'Arterial_lactate', 'input_total', 'input_4hourly',\n",
    "    'max_dose_vaso', 'output_total', 'output_4hourly', \n",
    "    #'gender', 're_admission', 'mechvent',\n",
    "    'Temp_C', 'GCS', 'SpO2', 'PTT', 'PT', 'INR'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(var_list))\n",
    "print(len(norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical variables\n",
    "\n",
    "def Normalizing(train_df, valid_df, test_df):\n",
    "\n",
    "    train_val = train_df[norm]\n",
    "    test_val = test_df[norm]\n",
    "    valid_val = valid_df[norm]\n",
    "\n",
    "    # Create and fit scalers\n",
    "    scaler = StandardScaler().fit(train_val.values)\n",
    "\n",
    "    # Transform \n",
    "    train_normalized = scaler.transform(train_val.values)\n",
    "\n",
    "    test_normalized = scaler.transform(test_val.values)\n",
    "\n",
    "    valid_normalized = scaler.transform(valid_val.values)\n",
    "\n",
    "    # Update dataframes with normalized values\n",
    "    train_df[norm] = train_normalized\n",
    "\n",
    "    test_df[norm] = test_normalized\n",
    "\n",
    "    valid_df[norm] = valid_normalized\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Prepare(df):\n",
    "    \n",
    "    \n",
    "    data = df.groupby(['patient_id'])\n",
    "    \n",
    "    X = []\n",
    "\n",
    "    y = []\n",
    "\n",
    "    ethnicity = []\n",
    "    \n",
    "    for _, frame in data:\n",
    "\n",
    "        X.append(frame[var_list].values)\n",
    "\n",
    "        y.append(frame.iloc[0]['label'])\n",
    "\n",
    "        ethnicity.append(frame.iloc[0]['ethnicity'])\n",
    "        \n",
    "    \n",
    "    return X, y, ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LactateData(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, l):\n",
    "        \n",
    "        self.X = torch.FloatTensor(X.astype('float'))\n",
    "        self.y = torch.FloatTensor(y.astype('float'))\n",
    "        self.l = l\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        l = self.l[index]\n",
    "        \n",
    "        return X, y, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.num_layers  = num_layers\n",
    "                \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc   = nn.Linear(hidden_size, num_classes) \n",
    "        \n",
    "        \n",
    "    def forward(self, x, seq_lengths):\n",
    "        \n",
    "        \n",
    "        packed_input = pack_padded_sequence(x, seq_lengths.to('cpu'), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        \n",
    "        out, (h, c) =  self.lstm(packed_input) \n",
    "        \n",
    "        \n",
    "        output = self.drop(h[-1])\n",
    "            \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size  = 44\n",
    "\n",
    "hidden_size = 256\n",
    "\n",
    "number_layers  = 1\n",
    "\n",
    "number_classes = 2\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 0.0008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, hidden_size, number_layers, number_classes):\n",
    "    \n",
    "    model = RNN(input_size, hidden_size, number_layers, number_classes)\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    \n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.normal_(m.bias.data)\n",
    "        \n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            else:\n",
    "                nn.init.normal_(param.data)\n",
    "                \n",
    "    elif isinstance(m, nn.GRU):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            else:\n",
    "                nn.init.normal_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize = (5, 5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 12)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize = 12,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = 12)\n",
    "    plt.xlabel('Predicted label', size = 12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration_curve(name, fig_index, true_labels, probs):\n",
    "    \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n",
    "\n",
    "    fig = plt.figure(fig_index, figsize=(8, 6))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    \n",
    "    frac_of_pos, mean_pred_value = calibration_curve(true_labels, probs, n_bins=10)\n",
    "\n",
    "    ax1.plot(mean_pred_value, frac_of_pos, \"s-\", label=f'{name}')\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title(f'Calibration plot ({name})')\n",
    "    \n",
    "    ax2.hist(probs, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n",
    "    ax2.set_xlabel(\"Mean predicted value\")\n",
    "    ax2.set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model With Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithTemperature(nn.Module):\n",
    "    \"\"\"\n",
    "    A thin decorator, which wraps a model with temperature scaling\n",
    "    model (nn.Module):\n",
    "        A classification neural network\n",
    "        NB: Output of the neural network should be the classification logits,\n",
    "            NOT the softmax (or log softmax)!\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, model):\n",
    "        \n",
    "        super(ModelWithTemperature, self).__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, length):\n",
    "        \n",
    "        logits = self.model(inputs, length)\n",
    "        \n",
    "        return self.temperature_scale(logits)\n",
    "    \n",
    "\n",
    "    def temperature_scale(self, logits):\n",
    "        \"\"\"\n",
    "        Perform temperature scaling on logits\n",
    "        \"\"\"\n",
    "        \n",
    "        # Expand temperature to match the size of logits\n",
    "        \n",
    "        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))\n",
    "        \n",
    "        \n",
    "        return logits / temperature\n",
    "    \n",
    "\n",
    "    # This function probably should live outside of this class, but whatever\n",
    "    def set_temperature(self, valid_loader):\n",
    "        \"\"\"\n",
    "        Tune the tempearature of the model (using the validation set).\n",
    "        We're going to set it to optimize NLL.\n",
    "        valid_loader (DataLoader): validation set loader\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cuda()\n",
    "        \n",
    "        nll_criterion = nn.CrossEntropyLoss().cuda()\n",
    "        \n",
    "        ece_criterion = _ECELoss().cuda()\n",
    "\n",
    "        # First: collect all the logits and labels for the validation set\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for inputs, labels, length in valid_loader:\n",
    "                \n",
    "                inputs = inputs.cuda()\n",
    "                length = length.cuda()\n",
    "                labels = labels.cuda()\n",
    "                \n",
    "                labels = labels.long()\n",
    "                \n",
    "                ###\n",
    "                seq_lengths, perm_idx = length.sort(0, descending=True)\n",
    "\n",
    "                inputs = inputs[perm_idx]\n",
    "                labels = labels[perm_idx]\n",
    "                ###\n",
    "                \n",
    "                logits = self.model(inputs, seq_lengths)\n",
    "                \n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(labels)\n",
    "                \n",
    "            logit = torch.cat(logits_list).cuda()\n",
    "            label = torch.cat(labels_list).cuda()\n",
    "\n",
    "        # Calculate NLL and ECE before temperature scaling\n",
    "        before_temperature_nll = nll_criterion(logit, label).item()\n",
    "        before_temperature_ece = ece_criterion(logit, label).item()\n",
    "        \n",
    "        # print('Before temperature - NLL: %.3f, ECE: %.3f' % (before_temperature_nll, before_temperature_ece))\n",
    "\n",
    "        # Next: optimize the temperature w.r.t. NLL\n",
    "        optimizer = optim.Adam([self.temperature], lr=0.001)\n",
    "\n",
    "        \n",
    "        def eval():\n",
    "            \n",
    "            loss = nll_criterion(self.temperature_scale(logit), label)\n",
    "            \n",
    "            loss.backward()\n",
    "        \n",
    "            return loss\n",
    "        \n",
    "        optimizer.step(eval)\n",
    "\n",
    "        \n",
    "        # Calculate NLL and ECE after temperature scaling\n",
    "        after_temperature_nll = nll_criterion(self.temperature_scale(logit), label).item()\n",
    "        after_temperature_ece = ece_criterion(self.temperature_scale(logit), label).item()\n",
    "        \n",
    "        #print('After temperature - NLL: %.3f, ECE: %.3f' % (after_temperature_nll, after_temperature_ece))\n",
    "        \n",
    "        #print('Optimal temperature: %.3f' % self.temperature.item())\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_temprature(self, valid_loader):\n",
    "        \n",
    "        self.cuda()\n",
    "        \n",
    "        nll_criterion = nn.CrossEntropyLoss().cuda()\n",
    "        \n",
    "        ece_criterion = _ECELoss().cuda()\n",
    "                \n",
    "        optimizer = optim.Adam([self.temperature], lr=0.001)\n",
    "        \n",
    "        \n",
    "        for epoch in range(25):\n",
    "            \n",
    "            logits_list = []\n",
    "            labels_list = []\n",
    "\n",
    "            self.train()\n",
    "            \n",
    "            for inputs, labels, length in valid_loader:\n",
    "                \n",
    "                inputs = inputs.cuda()\n",
    "                length = length.cuda()\n",
    "                labels = labels.cuda()\n",
    "                \n",
    "                labels = labels.long()\n",
    "                \n",
    "                ###\n",
    "                seq_lengths, perm_idx = length.sort(0, descending=True)\n",
    "\n",
    "                inputs = inputs[perm_idx]\n",
    "                labels = labels[perm_idx]\n",
    "                ###\n",
    "                \n",
    "                logits = self.model(inputs, seq_lengths)\n",
    "                \n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(labels)\n",
    "                \n",
    "            logit = torch.cat(logits_list).cuda()\n",
    "            label = torch.cat(labels_list).cuda()\n",
    "            \n",
    "            loss = nll_criterion(self.temperature_scale(logit), label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            after_temperature_nll = nll_criterion(self.temperature_scale(logit), label).item()\n",
    "            after_temperature_ece = ece_criterion(self.temperature_scale(logit), label).item()\n",
    "\n",
    "            #print(\"Epoch: {}/{} \".format(epoch+1, num_epochs), 'Temperature - NLL: %.3f, ECE: %.3f' % (after_temperature_nll, after_temperature_ece))\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ECELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the Expected Calibration Error of a model.\n",
    "    (This isn't necessary for temperature scaling, just a cool metric).\n",
    "\n",
    "    The input to this loss is the logits of a model, NOT the softmax scores.\n",
    "\n",
    "    This divides the confidence outputs into equally-sized interval bins.\n",
    "    In each bin, we compute the confidence gap:\n",
    "\n",
    "    bin_gap = | avg_confidence_in_bin - accuracy_in_bin |\n",
    "\n",
    "    We then return a weighted average of the gaps, based on the number\n",
    "    of samples in each bin\n",
    "\n",
    "    See: Naeini, Mahdi Pakdaman, Gregory F. Cooper, and Milos Hauskrecht.\n",
    "    \"Obtaining Well Calibrated Probabilities Using Bayesian Binning.\" AAAI.\n",
    "    2015.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_bins=10):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        \n",
    "        super(_ECELoss, self).__init__()\n",
    "        \n",
    "        bin_boundaries  = torch.linspace(0, 1, n_bins + 1)\n",
    "        \n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        \n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        \n",
    "        softmaxes = F.softmax(logits, dim=1)\n",
    "        \n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        \n",
    "        accuracies = predictions.eq(labels)\n",
    "\n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        \n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            \n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            \n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            \n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            \n",
    "            if prop_in_bin.item() > 0:\n",
    "                \n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                \n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                \n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indexing for real data only\n",
    "real_data = merged_data[merged_data['data_type'] == 'real']\n",
    "fake_data = merged_data[merged_data['data_type'] == 'fake']\n",
    "\n",
    "# Create a new column that combines 'label' and 'ethnicity' for stratification\n",
    "indexing_real = real_data[['patient_id', 'label', 'ethnicity']].groupby('patient_id').head(1)\n",
    "indexing_real['stratify_col'] = indexing_real['label'].astype(str) + '_' + indexing_real['ethnicity'].astype(int).astype(str)\n",
    "\n",
    "indexing_fake = fake_data[['patient_id', 'label', 'ethnicity']].groupby('patient_id').head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overall_ACC_ls_eth1 = []\n",
    "Overall_ACC_ls_eth0 = []\n",
    "Overall_ACC_ls = []\n",
    "Overall_PRC_ls_eth1 = []\n",
    "Overall_PRC_ls_eth0 = []\n",
    "Overall_PRC_ls = []\n",
    "Overall_REC_ls_eth1 = []\n",
    "Overall_REC_ls_eth0 = []\n",
    "Overall_REC_ls = []\n",
    "Overall_F1M_ls_eth1 = []\n",
    "Overall_F1M_ls_eth0 = []\n",
    "Overall_F1M_ls = []\n",
    "Overall_PPV_ls_eth1 = []\n",
    "Overall_PPV_ls_eth0 = []\n",
    "Overall_PPV_ls = []\n",
    "Overall_NPV_ls_eth1 = []\n",
    "Overall_NPV_ls_eth0 = []\n",
    "Overall_NPV_ls = []\n",
    "Overall_SEN_ls_eth1 = []\n",
    "Overall_SEN_ls_eth0 = []\n",
    "Overall_SEN_ls = []\n",
    "Overall_SPE_ls_eth1 = []\n",
    "Overall_SPE_ls_eth0 = []\n",
    "Overall_SPE_ls = []\n",
    "Overall_MCC_ls_eth1 = []\n",
    "Overall_MCC_ls_eth0 = []\n",
    "Overall_MCC_ls = []\n",
    "Overall_AUC_ls_eth1 = []\n",
    "Overall_AUC_ls_eth0 = []\n",
    "Overall_AUC_ls = []\n",
    "Overall_AP_ls_eth1 = []\n",
    "Overall_AP_ls_eth0 = []\n",
    "Overall_AP_ls = []\n",
    "No_Skill_ls_eth1 = []\n",
    "No_Skill_ls_eth0 = []\n",
    "No_Skill_ls = []\n",
    "tprs_ls_eth1 = []\n",
    "tprs_ls_eth0 = []\n",
    "tprs_ls = []\n",
    "aucs_ls_eth1 = []\n",
    "aucs_ls_eth0 = []\n",
    "aucs_ls = []\n",
    "mean_fpr_ls_eth1 = np.linspace(0, 1, 100)\n",
    "mean_fpr_ls_eth0 = np.linspace(0, 1, 100)\n",
    "mean_fpr_ls = np.linspace(0, 1, 100)\n",
    "prs_ls_eth1 = []\n",
    "prs_ls_eth0 = []\n",
    "prs_ls = []\n",
    "ap_ls_eth1 = []\n",
    "ap_ls_eth0 = []\n",
    "ap_ls = []\n",
    "mean_recall_ls_eth1 = np.linspace(0, 1, 100)\n",
    "mean_recall_ls_eth0 = np.linspace(0, 1, 100)\n",
    "mean_recall_ls = np.linspace(0, 1, 100)\n",
    "cal_prob_ls_eth1 = []\n",
    "cal_prob_ls_eth0 = []\n",
    "cal_prob_ls = []\n",
    "cal_label_ls_eth1 = []\n",
    "cal_label_ls_eth0 = []\n",
    "cal_label_ls = []\n",
    "\n",
    "Total_attribute = []\n",
    "\n",
    "Total_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=3, random_state=i*5, shuffle=True)\n",
    "\n",
    "    for train_index, test_index in skf.split(indexing_real.patient_id, indexing_real.stratify_col):\n",
    "\n",
    "\n",
    "        train = indexing_real.iloc[train_index]\n",
    "\n",
    "        test  = indexing_real.iloc[test_index]\n",
    "        \n",
    "        \n",
    "        train, valid = train_test_split(train, stratify= train.label, test_size= 0.20, random_state= 42)\n",
    "\n",
    "\n",
    "        lactate_train_id = train.patient_id.unique()\n",
    "        \n",
    "        lactate_valid_id = valid.patient_id.unique()\n",
    "\n",
    "        lactate_test_id  = test.patient_id.unique()\n",
    "\n",
    "\n",
    "        train_df = merged_data[merged_data['patient_id'].isin(lactate_train_id)]\n",
    "        \n",
    "        valid_df = merged_data[merged_data['patient_id'].isin(lactate_valid_id)]\n",
    "\n",
    "        test_df  = merged_data[merged_data['patient_id'].isin(lactate_test_id)]\n",
    "\n",
    "\n",
    "        train_df_normilized, valid_df_normilized, test_df_normalized = Normalizing(train_df, valid_df, test_df)\n",
    "\n",
    "\n",
    "        X_train, y_train, ethnicity_train = Data_Prepare(train_df_normilized)\n",
    "        \n",
    "        X_valid, y_valid, ethnicity_valid = Data_Prepare(valid_df_normilized)\n",
    "\n",
    "        X_test , y_test, ethnicity_test  = Data_Prepare(test_df_normalized)\n",
    "\n",
    "\n",
    "        X_data_train = np.array(X_train)\n",
    "        \n",
    "        X_data_valid = np.array(X_valid)\n",
    "        \n",
    "        X_data_test  = np.array(X_test)\n",
    "        \n",
    "        \n",
    "        y_train = np.array(y_train)\n",
    "        \n",
    "        y_valid = np.array(y_valid)\n",
    "        \n",
    "        y_test  = np.array(y_test)\n",
    "\n",
    "\n",
    "        ethnicity_test = np.array(ethnicity_test)\n",
    "\n",
    "\n",
    "        X_len_train = np.full(len(y_train), 14)\n",
    "        X_len_valid = np.full(len(y_valid), 14)\n",
    "        X_len_test = np.full(len(y_test), 14)\n",
    "\n",
    "\n",
    "        train_data = LactateData(X_data_train, y_train, X_len_train)\n",
    "        valid_data = LactateData(X_data_valid, y_valid, X_len_valid)\n",
    "        test_data  = LactateData(X_data_test , y_test , X_len_test)\n",
    "        \n",
    "        \n",
    "        train_class_sample_count = torch.tensor([(torch.tensor(y_train) == t).sum() for t in torch.unique(torch.tensor(y_train), sorted=True)])\n",
    "        train_weight  = 1 / train_class_sample_count.float()\n",
    "        train_samples_weight = torch.tensor([train_weight[i] for i in torch.tensor(y_train).long()])\n",
    "        train_sampler = WeightedRandomSampler(train_samples_weight, len(train_samples_weight))\n",
    "\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False, sampler=train_sampler, **kwargs)\n",
    "        valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        test_loader  = DataLoader(test_data , batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        \n",
    "        \n",
    "        model = build_model(input_size, hidden_size, number_layers, number_classes)\n",
    "        \n",
    "        model.apply(weight_init)\n",
    "        \n",
    "        \n",
    "        class_weight = torch.Tensor([0.65, 0.35])\n",
    "        class_weight = class_weight.to(device)\n",
    "        criterion    = nn.CrossEntropyLoss(weight= class_weight)\n",
    "        \n",
    "        l1_crit   = nn.L1Loss(size_average=False)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        ##########################################################################\n",
    "        \n",
    "        # Initialize early stopping\n",
    "        early_stopping = EarlyStopping(patience=10, verbose=False)\n",
    "\n",
    "        train_losses = []\n",
    "        valid_losses = []  # Added to track validation losses\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0\n",
    "            model.train()\n",
    "\n",
    "            for x, y, l in train_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                l = l.to(device)\n",
    "                y = y.long()\n",
    "\n",
    "                seq_lengths, perm_idx = l.sort(0, descending=True)\n",
    "                x = x[perm_idx]\n",
    "                y = y[perm_idx]\n",
    "\n",
    "                outputs = model(x, seq_lengths)\n",
    "                entropy_loss = criterion(outputs, y)\n",
    "\n",
    "                l1_loss_ = 0\n",
    "                for param in model.lstm.parameters():\n",
    "                    l1_loss_ += l1_crit(param, target=torch.zeros_like(param))\n",
    "                factor_1 = 0.0006\n",
    "                l1_loss = factor_1 * l1_loss_\n",
    "\n",
    "                loss = entropy_loss + l1_loss \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()   \n",
    "\n",
    "                train_loss += loss.item()   \n",
    "            \n",
    "            train_losses.append(train_loss/len(train_loader))\n",
    "\n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for x, y, l in valid_loader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    l = l.to(device)\n",
    "                    y = y.long()\n",
    "\n",
    "                    seq_lengths, perm_idx = l.sort(0, descending=True)\n",
    "                    x = x[perm_idx]\n",
    "                    y = y[perm_idx]\n",
    "\n",
    "                    outputs = model(x, seq_lengths)\n",
    "                    loss = criterion(outputs, y)  # Using only entropy loss for validation\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "            valid_loss = valid_loss / len(valid_loader)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            #print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "            # Early stopping\n",
    "            early_stopping(valid_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        print(f\"TRAIN ENDED (epochs = {epoch+1})\")\n",
    "        ##########################################################################\n",
    "        \n",
    "        temp_model = ModelWithTemperature(model)\n",
    "        \n",
    "        temp_model.set_temperature(valid_loader)\n",
    "        \n",
    "        temp_model.train_temprature(valid_loader)    \n",
    "       \n",
    "        ##########################################################################\n",
    "        \n",
    "        test_losses = []\n",
    "            \n",
    "        temp_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            total = 0\n",
    "            \n",
    "            test_loss = 0\n",
    "\n",
    "            test_labels = []\n",
    "\n",
    "            test_probs  = []\n",
    "\n",
    "            for x, y, l in test_loader:\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                l = l.to(device)\n",
    "\n",
    "                y = y.long()\n",
    "\n",
    "\n",
    "                ###\n",
    "                seq_lengths, perm_idx = l.sort(0, descending=True)\n",
    "\n",
    "                x = x[perm_idx]\n",
    "                y = y[perm_idx]\n",
    "                ###\n",
    "\n",
    "\n",
    "                output = temp_model(x, seq_lengths)\n",
    "                \n",
    "                entropy_loss = criterion(output, y)\n",
    "                \n",
    "                \n",
    "                l1_loss_ = 0\n",
    "                \n",
    "                for param in model.lstm.parameters():\n",
    "            \n",
    "                    l1_loss_ += l1_crit(param, target=torch.zeros_like(param))\n",
    "            \n",
    "                factor = 0.0006\n",
    "        \n",
    "                l1_loss = factor * l1_loss_\n",
    "\n",
    "                                \n",
    "                loss = entropy_loss + l1_loss \n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                \n",
    "\n",
    "                outputs = nn.Softmax()(output)\n",
    "\n",
    "                prediction = outputs.detach().cpu().numpy()\n",
    "\n",
    "                prediction = prediction[:,1]\n",
    "\n",
    "                test_labels.append(y.detach().cpu().numpy())\n",
    "\n",
    "                test_probs.append(prediction)\n",
    "                \n",
    "                \n",
    "            test_losses.append(test_loss/len(test_loader))\n",
    "        \n",
    "            #print(\"Test loss: {:0.4f} \".format(test_loss/len(test_loader)))\n",
    "        \n",
    "            #print(\"TEST ENDED\")\n",
    "            \n",
    "            \n",
    "            test_labels = [l for labels in test_labels for l in labels]\n",
    "        \n",
    "            test_probs  = [p for probs  in test_probs  for p in probs]\n",
    "                        \n",
    "        ##########################################################################\n",
    "            \n",
    "\n",
    "        pred_probs_eth0 = np.array(test_probs)[ethnicity_test == 0]\n",
    "        true_labels_eth0 = np.array(test_labels)[ethnicity_test == 0]\n",
    "\n",
    "        pred_probs_eth1 = np.array(test_probs)[ethnicity_test == 1]\n",
    "        true_labels_eth1 = np.array(test_labels)[ethnicity_test == 1]\n",
    "\n",
    "        # For overall (without ethnicity specification)\n",
    "        cal_label_ls.append(test_labels)\n",
    "        cal_prob_ls.append(test_probs)\n",
    "\n",
    "        TN, FP, FN, TP = confusion_matrix(test_labels, np.array(test_probs).round()).ravel()\n",
    "\n",
    "        ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        PRC = (TP)/(TP+FP)\n",
    "        REC = (TP)/(TP+FN)\n",
    "        F1M = (2*PRC*REC)/(PRC+REC)\n",
    "        PPV = TP/(TP+FP)\n",
    "        NPV = TN/(TN+FN)\n",
    "        SEN = (TP)/(TP+FN)\n",
    "        SPE = (TN)/(TN+FP)\n",
    "        MCC = matthews_corrcoef(test_labels, np.array(test_probs).round())\n",
    "        AUC = roc_auc_score(test_labels, test_probs)\n",
    "        AP = average_precision_score(test_labels, test_probs)\n",
    "\n",
    "        Overall_ACC_ls.append(ACC)\n",
    "        Overall_PRC_ls.append(PRC)\n",
    "        Overall_REC_ls.append(REC)\n",
    "        Overall_F1M_ls.append(F1M)\n",
    "        Overall_PPV_ls.append(PPV)\n",
    "        Overall_NPV_ls.append(NPV)\n",
    "        Overall_SEN_ls.append(SEN)\n",
    "        Overall_SPE_ls.append(SPE)\n",
    "        Overall_MCC_ls.append(MCC)\n",
    "        Overall_AUC_ls.append(AUC)\n",
    "        Overall_AP_ls.append(AP)\n",
    "\n",
    "        fpr_lstm, tpr_lstm, thresholds_lstm = roc_curve(test_labels, test_probs)\n",
    "        tprs_ls.append(interp(mean_fpr_ls, fpr_lstm, tpr_lstm))\n",
    "        tprs_ls[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr_lstm, tpr_lstm)\n",
    "        aucs_ls.append(roc_auc)\n",
    "\n",
    "        no_skill = len([lab for lab in test_labels if lab == 1]) / len(test_labels)\n",
    "        No_Skill_ls.append(no_skill)\n",
    "\n",
    "        precision_lstm, recall_lstm, threshold_lstm = precision_recall_curve(test_labels, test_probs)\n",
    "        prs_ls.append(interp(mean_recall_ls, precision_lstm, recall_lstm))\n",
    "        pr_ap = auc(recall_lstm, precision_lstm)\n",
    "        ap_ls.append(pr_ap)\n",
    "\n",
    "\n",
    "        # For eth1\n",
    "        cal_label_ls_eth1.append(true_labels_eth1)\n",
    "        cal_prob_ls_eth1.append(pred_probs_eth1)\n",
    "\n",
    "        TN_eth1, FP_eth1, FN_eth1, TP_eth1 = confusion_matrix(true_labels_eth1, pred_probs_eth1.round()).ravel()\n",
    "\n",
    "        ACC_eth1 = (TP_eth1+TN_eth1)/(TP_eth1+FP_eth1+FN_eth1+TN_eth1)\n",
    "        PRC_eth1 = (TP_eth1)/(TP_eth1+FP_eth1)\n",
    "        REC_eth1 = (TP_eth1)/(TP_eth1+FN_eth1)\n",
    "        F1M_eth1 = (2*PRC_eth1*REC_eth1)/(PRC_eth1+REC_eth1)\n",
    "        PPV_eth1 = TP_eth1/(TP_eth1+FP_eth1)\n",
    "        NPV_eth1 = TN_eth1/(TN_eth1+FN_eth1)\n",
    "        SEN_eth1 = (TP_eth1)/(TP_eth1+FN_eth1)\n",
    "        SPE_eth1 = (TN_eth1)/(TN_eth1+FP_eth1)\n",
    "        MCC_eth1 = matthews_corrcoef(true_labels_eth1, pred_probs_eth1.round())\n",
    "        AUC_eth1 = roc_auc_score(true_labels_eth1, pred_probs_eth1)\n",
    "        AP_eth1 = average_precision_score(true_labels_eth1, pred_probs_eth1)\n",
    "\n",
    "        Overall_ACC_ls_eth1.append(ACC_eth1)\n",
    "        Overall_PRC_ls_eth1.append(PRC_eth1)\n",
    "        Overall_REC_ls_eth1.append(REC_eth1)\n",
    "        Overall_F1M_ls_eth1.append(F1M_eth1)\n",
    "        Overall_PPV_ls_eth1.append(PPV_eth1)\n",
    "        Overall_NPV_ls_eth1.append(NPV_eth1)\n",
    "        Overall_SEN_ls_eth1.append(SEN_eth1)\n",
    "        Overall_SPE_ls_eth1.append(SPE_eth1)\n",
    "        Overall_MCC_ls_eth1.append(MCC_eth1)\n",
    "        Overall_AUC_ls_eth1.append(AUC_eth1)\n",
    "        Overall_AP_ls_eth1.append(AP_eth1)\n",
    "\n",
    "        fpr_lstm_eth1, tpr_lstm_eth1, thresholds_lstm_eth1 = roc_curve(true_labels_eth1, pred_probs_eth1)\n",
    "        tprs_ls_eth1.append(interp(mean_fpr_ls_eth1, fpr_lstm_eth1, tpr_lstm_eth1))\n",
    "        tprs_ls_eth1[-1][0] = 0.0\n",
    "        roc_auc_eth1 = auc(fpr_lstm_eth1, tpr_lstm_eth1)\n",
    "        aucs_ls_eth1.append(roc_auc_eth1)\n",
    "\n",
    "        no_skill_eth1 = len([lab for lab in true_labels_eth1 if lab == 1]) / len(true_labels_eth1)\n",
    "        No_Skill_ls_eth1.append(no_skill_eth1)\n",
    "\n",
    "        precision_lstm_eth1, recall_lstm_eth1, threshold_lstm_eth1 = precision_recall_curve(true_labels_eth1, pred_probs_eth1)\n",
    "        prs_ls_eth1.append(interp(mean_recall_ls_eth1, precision_lstm_eth1, recall_lstm_eth1))\n",
    "        pr_ap_eth1 = auc(recall_lstm_eth1, precision_lstm_eth1)\n",
    "        ap_ls_eth1.append(pr_ap_eth1)\n",
    "\n",
    "        # For eth0\n",
    "        cal_label_ls_eth0.append(true_labels_eth0)\n",
    "        cal_prob_ls_eth0.append(pred_probs_eth0)\n",
    "\n",
    "        TN_eth0, FP_eth0, FN_eth0, TP_eth0 = confusion_matrix(true_labels_eth0, pred_probs_eth0.round()).ravel()\n",
    "\n",
    "        ACC_eth0 = (TP_eth0+TN_eth0)/(TP_eth0+FP_eth0+FN_eth0+TN_eth0)\n",
    "        PRC_eth0 = (TP_eth0)/(TP_eth0+FP_eth0)\n",
    "        REC_eth0 = (TP_eth0)/(TP_eth0+FN_eth0)\n",
    "        F1M_eth0 = (2*PRC_eth0*REC_eth0)/(PRC_eth0+REC_eth0)\n",
    "        PPV_eth0 = TP_eth0/(TP_eth0+FP_eth0)\n",
    "        NPV_eth0 = TN_eth0/(TN_eth0+FN_eth0)\n",
    "        SEN_eth0 = (TP_eth0)/(TP_eth0+FN_eth0)\n",
    "        SPE_eth0 = (TN_eth0)/(TN_eth0+FP_eth0)\n",
    "        MCC_eth0 = matthews_corrcoef(true_labels_eth0, pred_probs_eth0.round())\n",
    "        AUC_eth0 = roc_auc_score(true_labels_eth0, pred_probs_eth0)\n",
    "        AP_eth0 = average_precision_score(true_labels_eth0, pred_probs_eth0)\n",
    "\n",
    "        Overall_ACC_ls_eth0.append(ACC_eth0)\n",
    "        Overall_PRC_ls_eth0.append(PRC_eth0)\n",
    "        Overall_REC_ls_eth0.append(REC_eth0)\n",
    "        Overall_F1M_ls_eth0.append(F1M_eth0)\n",
    "        Overall_PPV_ls_eth0.append(PPV_eth0)\n",
    "        Overall_NPV_ls_eth0.append(NPV_eth0)\n",
    "        Overall_SEN_ls_eth0.append(SEN_eth0)\n",
    "        Overall_SPE_ls_eth0.append(SPE_eth0)\n",
    "        Overall_MCC_ls_eth0.append(MCC_eth0)\n",
    "        Overall_AUC_ls_eth0.append(AUC_eth0)\n",
    "        Overall_AP_ls_eth0.append(AP_eth0)\n",
    "\n",
    "        fpr_lstm_eth0, tpr_lstm_eth0, thresholds_lstm_eth0 = roc_curve(true_labels_eth0, pred_probs_eth0)\n",
    "        tprs_ls_eth0.append(interp(mean_fpr_ls_eth0, fpr_lstm_eth0, tpr_lstm_eth0))\n",
    "        tprs_ls_eth0[-1][0] = 0.0\n",
    "        roc_auc_eth0 = auc(fpr_lstm_eth0, tpr_lstm_eth0)\n",
    "        aucs_ls_eth0.append(roc_auc_eth0)\n",
    "\n",
    "        no_skill_eth0 = len([lab for lab in true_labels_eth0 if lab == 1]) / len(true_labels_eth0)\n",
    "        No_Skill_ls_eth0.append(no_skill_eth0)\n",
    "\n",
    "        precision_lstm_eth0, recall_lstm_eth0, threshold_lstm_eth0 = precision_recall_curve(true_labels_eth0, pred_probs_eth0)\n",
    "        prs_ls_eth0.append(interp(mean_recall_ls_eth0, precision_lstm_eth0, recall_lstm_eth0))\n",
    "        pr_ap_eth0 = auc(recall_lstm_eth0, precision_lstm_eth0)\n",
    "        ap_ls_eth0.append(pr_ap_eth0)\n",
    "        \n",
    "        ##########################################################################\n",
    "            \n",
    "        IG = IntegratedGradients(temp_model)\n",
    "        \n",
    "        temp_model.train()\n",
    "    \n",
    "        for x, y, l in test_loader:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            l = l.to(device)\n",
    "\n",
    "            y = y.long()\n",
    "\n",
    "\n",
    "            ###\n",
    "            seq_lengths, perm_idx = l.sort(0, descending=True)\n",
    "\n",
    "            x = x[perm_idx]\n",
    "            y = y[perm_idx]\n",
    "            ###\n",
    "\n",
    "\n",
    "            attribute = IG.attribute(x, additional_forward_args=seq_lengths, target=0)\n",
    "\n",
    "            Total_attribute.append(attribute.detach().cpu().numpy())\n",
    "            \n",
    "        \n",
    "        Total_features.append(X_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ethnicity: 0\")\n",
    "\n",
    "print(f\"Mean ACC: {np.mean(Overall_ACC_ls_eth0):.4f}\")\n",
    "print(f\"Mean PRC: {np.mean(Overall_PRC_ls_eth0):.4f}\")\n",
    "print(f\"Mean REC: {np.mean(Overall_REC_ls_eth0):.4f}\")\n",
    "print(f\"Mean F1M: {np.mean(Overall_F1M_ls_eth0):.4f}\")\n",
    "print(f\"Mean PPV: {np.mean(Overall_PPV_ls_eth0):.4f}\")\n",
    "print(f\"Mean NPV: {np.mean(Overall_NPV_ls_eth0):.4f}\")\n",
    "print(f\"Mean SEN: {np.mean(Overall_SEN_ls_eth0):.4f}\")\n",
    "print(f\"Mean SPE: {np.mean(Overall_SPE_ls_eth0):.4f}\")\n",
    "print(f\"Mean MCC: {np.mean(Overall_MCC_ls_eth0):.4f}\")\n",
    "print(f\"Mean AUC: {np.mean(Overall_AUC_ls_eth0):.4f}\")\n",
    "print(f\"Mean AP: {np.mean(Overall_AP_ls_eth0):.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Ethnicity: 1\")\n",
    "\n",
    "print(f\"Mean ACC: {np.mean(Overall_ACC_ls_eth1):.4f}\")\n",
    "print(f\"Mean PRC: {np.mean(Overall_PRC_ls_eth1):.4f}\")\n",
    "print(f\"Mean REC: {np.mean(Overall_REC_ls_eth1):.4f}\")\n",
    "print(f\"Mean F1M: {np.mean(Overall_F1M_ls_eth1):.4f}\")\n",
    "print(f\"Mean PPV: {np.mean(Overall_PPV_ls_eth1):.4f}\")\n",
    "print(f\"Mean NPV: {np.mean(Overall_NPV_ls_eth1):.4f}\")\n",
    "print(f\"Mean SEN: {np.mean(Overall_SEN_ls_eth1):.4f}\")\n",
    "print(f\"Mean SPE: {np.mean(Overall_SPE_ls_eth1):.4f}\")\n",
    "print(f\"Mean MCC: {np.mean(Overall_MCC_ls_eth1):.4f}\")\n",
    "print(f\"Mean AUC: {np.mean(Overall_AUC_ls_eth1):.4f}\")\n",
    "print(f\"Mean AP: {np.mean(Overall_AP_ls_eth1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ethnicity: 0\")\n",
    "\n",
    "print(f\"Std ACC: {np.std(Overall_ACC_ls_eth0):.4f}\")\n",
    "print(f\"Std PRC: {np.std(Overall_PRC_ls_eth0):.4f}\")\n",
    "print(f\"Std REC: {np.std(Overall_REC_ls_eth0):.4f}\")\n",
    "print(f\"Std F1M: {np.std(Overall_F1M_ls_eth0):.4f}\")\n",
    "print(f\"Std PPV: {np.std(Overall_PPV_ls_eth0):.4f}\")\n",
    "print(f\"Std NPV: {np.std(Overall_NPV_ls_eth0):.4f}\")\n",
    "print(f\"Std SEN: {np.std(Overall_SEN_ls_eth0):.4f}\")\n",
    "print(f\"Std SPE: {np.std(Overall_SPE_ls_eth0):.4f}\")\n",
    "print(f\"Std MCC: {np.std(Overall_MCC_ls_eth0):.4f}\")\n",
    "print(f\"Std AUC: {np.std(Overall_AUC_ls_eth0):.4f}\")\n",
    "print(f\"Std AP: {np.std(Overall_AP_ls_eth0):.4f}\")\n",
    "\n",
    "print(\"\\n\")  # Add a blank line for separation\n",
    "\n",
    "print(\"Ethnicity: 1\")\n",
    "\n",
    "print(f\"Std ACC: {np.std(Overall_ACC_ls_eth1):.4f}\")\n",
    "print(f\"Std PRC: {np.std(Overall_PRC_ls_eth1):.4f}\")\n",
    "print(f\"Std REC: {np.std(Overall_REC_ls_eth1):.4f}\")\n",
    "print(f\"Std F1M: {np.std(Overall_F1M_ls_eth1):.4f}\")\n",
    "print(f\"Std PPV: {np.std(Overall_PPV_ls_eth1):.4f}\")\n",
    "print(f\"Std NPV: {np.std(Overall_NPV_ls_eth1):.4f}\")\n",
    "print(f\"Std SEN: {np.std(Overall_SEN_ls_eth1):.4f}\")\n",
    "print(f\"Std SPE: {np.std(Overall_SPE_ls_eth1):.4f}\")\n",
    "print(f\"Std MCC: {np.std(Overall_MCC_ls_eth1):.4f}\")\n",
    "print(f\"Std AUC: {np.std(Overall_AUC_ls_eth1):.4f}\")\n",
    "print(f\"Std AP: {np.std(Overall_AP_ls_eth1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['ACC', 'PRC', 'REC', 'F1M', 'PPV', 'NPV', 'SEN', 'SPE', 'MCC', 'AUC', 'AP']\n",
    "\n",
    "print(\"Results for original data:\\n\")\n",
    "\n",
    "print(f\"{'Metric':<7}{'Overall (Mean ± Std)':<30}{'Ethnicity 0 (Mean ± Std)':<30}{'Ethnicity 1 (Mean ± Std)':<30}\")\n",
    "\n",
    "print('-' * 96)\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "    eth0_mean = np.nanmean(globals()[f'Overall_{metric}_ls_eth0'])\n",
    "    eth0_std = np.nanstd(globals()[f'Overall_{metric}_ls_eth0'])\n",
    "    eth1_mean = np.nanmean(globals()[f'Overall_{metric}_ls_eth1'])\n",
    "    eth1_std = np.nanstd(globals()[f'Overall_{metric}_ls_eth1'])\n",
    "\n",
    "    full_mean = np.nanmean(globals()[f'Overall_{metric}_ls'])\n",
    "    full_std = np.nanstd(globals()[f'Overall_{metric}_ls'])\n",
    "    \n",
    "    print(f\"{metric:<7}{full_mean:.4f} ± {full_std:<21.4f}{eth0_mean:.4f} ± {eth0_std:<21.4f}{eth1_mean:.4f} ± {eth1_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "metrics = ['ACC', 'PRC', 'REC', 'F1M', 'PPV', 'NPV', 'SEN', 'SPE', 'MCC', 'AUC', 'AP']\n",
    "\n",
    "print(\"Results for original data:\\n\")\n",
    "\n",
    "print(f\"{'Metric':<7}{'Full data (Mean [95% CI])':<35}{'Ethnicity 0 (Mean [95% CI])':<35}{'Ethnicity 1 (Mean [95% CI])':<35}\")\n",
    "\n",
    "print('-' * 77)\n",
    "\n",
    "for metric in metrics:\n",
    "    eth0_data = globals()[f'Overall_{metric}_ls_eth0']\n",
    "    eth1_data = globals()[f'Overall_{metric}_ls_eth1']\n",
    "    full_data = globals()[f'Overall_{metric}_ls']\n",
    "    \n",
    "    eth0_mean = np.nanmean(eth0_data)\n",
    "    eth1_mean = np.nanmean(eth1_data)\n",
    "    full_mean = np.nanmean(full_data)\n",
    "    \n",
    "    eth0_sem = stats.sem(eth0_data, nan_policy='omit')\n",
    "    eth1_sem = stats.sem(eth1_data, nan_policy='omit')\n",
    "    full_sem = stats.sem(full_data, nan_policy='omit')\n",
    "    \n",
    "    eth0_ci = eth0_mean - 1.96*eth0_sem, eth0_mean + 1.96*eth0_sem\n",
    "    eth1_ci = eth1_mean - 1.96*eth1_sem, eth1_mean + 1.96*eth1_sem\n",
    "    full_ci = full_mean - 1.96*full_sem, full_mean + 1.96*full_sem\n",
    "    \n",
    "    print(f\"{metric:<7}{full_mean:.4f} [{full_ci[0]:.4f}, {full_ci[1]:.4f}]{' '*5}{eth0_mean:.4f} [{eth0_ci[0]:.4f}, {eth0_ci[1]:.4f}]{' '*5}{eth1_mean:.4f} [{eth1_ci[0]:.4f}, {eth1_ci[1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for Ethnicity 0\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='green', label='Random', alpha=.8)\n",
    "\n",
    "mean_tpr_ls_eth0 = np.mean(tprs_ls_eth0, axis=0)\n",
    "mean_tpr_ls_eth0[-1] = 1.0\n",
    "mean_auc_ls_eth0 = auc(mean_fpr_ls_eth0, mean_tpr_ls_eth0)\n",
    "std_auc_ls_eth0 = np.std(aucs_ls_eth0)\n",
    "plt.plot(mean_fpr_ls_eth0, mean_tpr_ls_eth0, color='r', \n",
    "         label=r'LSTM Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_ls_eth0, std_auc_ls_eth0), \n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr_ls_eth0 = np.std(tprs_ls_eth0, axis=0)\n",
    "tprs_upper_ls_eth0 = np.minimum(mean_tpr_ls_eth0 + std_tpr_ls_eth0, 1)\n",
    "tprs_lower_ls_eth0 = np.maximum(mean_tpr_ls_eth0 - std_tpr_ls_eth0, 0)\n",
    "plt.fill_between(mean_fpr_ls_eth0, tprs_lower_ls_eth0, tprs_upper_ls_eth0, color='grey', alpha=.2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Ethnicity 0')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Plot for Ethnicity 1\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='green', label='Random', alpha=.8)\n",
    "\n",
    "mean_tpr_ls_eth1 = np.mean(tprs_ls_eth1, axis=0)\n",
    "mean_tpr_ls_eth1[-1] = 1.0\n",
    "mean_auc_ls_eth1 = auc(mean_fpr_ls_eth1, mean_tpr_ls_eth1)\n",
    "std_auc_ls_eth1 = np.std(aucs_ls_eth1)\n",
    "plt.plot(mean_fpr_ls_eth1, mean_tpr_ls_eth1, color='b', \n",
    "         label=r'LSTM Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_ls_eth1, std_auc_ls_eth1), \n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr_ls_eth1 = np.std(tprs_ls_eth1, axis=0)\n",
    "tprs_upper_ls_eth1 = np.minimum(mean_tpr_ls_eth1 + std_tpr_ls_eth1, 1)\n",
    "tprs_lower_ls_eth1 = np.maximum(mean_tpr_ls_eth1 - std_tpr_ls_eth1, 0)\n",
    "plt.fill_between(mean_fpr_ls_eth1, tprs_lower_ls_eth1, tprs_upper_ls_eth1, color='grey', alpha=.2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Ethnicity 1')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for Ethnicity 0\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([0, 1], [np.mean(No_Skill_ls_eth0), np.mean(No_Skill_ls_eth0)], linestyle='--', lw=2, color='green', label='Random', alpha=.8)\n",
    "\n",
    "mean_prs_ls_eth0 = np.mean(prs_ls_eth0, axis=0)\n",
    "mean_ap_ls_eth0 = auc(mean_recall_ls_eth0, mean_prs_ls_eth0)\n",
    "std_ap_ls_eth0 = np.std(ap_ls_eth0)\n",
    "plt.plot(mean_recall_ls_eth0, mean_prs_ls_eth0, color='r', \n",
    "         label=r'Mean (AP = %0.3f $\\pm$ %0.2f)' % (mean_ap_ls_eth0, std_ap_ls_eth0), \n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_prs_ls_eth0 = np.std(prs_ls_eth0, axis=0)\n",
    "prs_upper_ls_eth0 = np.minimum(mean_prs_ls_eth0 + std_prs_ls_eth0, 1)\n",
    "prs_lower_ls_eth0 = np.maximum(mean_prs_ls_eth0 - std_prs_ls_eth0, 0)\n",
    "plt.fill_between(mean_recall_ls_eth0, prs_lower_ls_eth0, prs_upper_ls_eth0, color='grey', alpha=.2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve - Ethnicity 0')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot for Ethnicity 1\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([0, 1], [np.mean(No_Skill_ls_eth1), np.mean(No_Skill_ls_eth1)], linestyle='--', lw=2, color='green', label='Random', alpha=.8)\n",
    "\n",
    "mean_prs_ls_eth1 = np.mean(prs_ls_eth1, axis=0)\n",
    "mean_ap_ls_eth1 = auc(mean_recall_ls_eth1, mean_prs_ls_eth1)\n",
    "std_ap_ls_eth1 = np.std(ap_ls_eth1)\n",
    "plt.plot(mean_recall_ls_eth1, mean_prs_ls_eth1, color='b', \n",
    "         label=r'Mean (AP = %0.3f $\\pm$ %0.2f)' % (mean_ap_ls_eth1, std_ap_ls_eth1), \n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_prs_ls_eth1 = np.std(prs_ls_eth1, axis=0)\n",
    "prs_upper_ls_eth1 = np.minimum(mean_prs_ls_eth1 + std_prs_ls_eth1, 1)\n",
    "prs_lower_ls_eth1 = np.maximum(mean_prs_ls_eth1 - std_prs_ls_eth1, 0)\n",
    "plt.fill_between(mean_recall_ls_eth1, prs_lower_ls_eth1, prs_upper_ls_eth1, color='grey', alpha=.2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve - Ethnicity 1')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_real = {\n",
    "    'Overall_ACC_ls_eth1': Overall_ACC_ls_eth1,\n",
    "    'Overall_ACC_ls_eth0': Overall_ACC_ls_eth0,\n",
    "    'Overall_ACC_ls': Overall_ACC_ls,\n",
    "    'Overall_PRC_ls_eth1': Overall_PRC_ls_eth1,\n",
    "    'Overall_PRC_ls_eth0': Overall_PRC_ls_eth0,\n",
    "    'Overall_PRC_ls': Overall_PRC_ls,\n",
    "    'Overall_REC_ls_eth1': Overall_REC_ls_eth1,\n",
    "    'Overall_REC_ls_eth0': Overall_REC_ls_eth0,\n",
    "    'Overall_REC_ls': Overall_REC_ls,\n",
    "    'Overall_F1M_ls_eth1': Overall_F1M_ls_eth1,\n",
    "    'Overall_F1M_ls_eth0': Overall_F1M_ls_eth0,\n",
    "    'Overall_F1M_ls': Overall_F1M_ls,\n",
    "    'Overall_PPV_ls_eth1': Overall_PPV_ls_eth1,\n",
    "    'Overall_PPV_ls_eth0': Overall_PPV_ls_eth0,\n",
    "    'Overall_PPV_ls': Overall_PPV_ls,\n",
    "    'Overall_NPV_ls_eth1': Overall_NPV_ls_eth1,\n",
    "    'Overall_NPV_ls_eth0': Overall_NPV_ls_eth0,\n",
    "    'Overall_NPV_ls': Overall_NPV_ls,\n",
    "    'Overall_SEN_ls_eth1': Overall_SEN_ls_eth1,\n",
    "    'Overall_SEN_ls_eth0': Overall_SEN_ls_eth0,\n",
    "    'Overall_SEN_ls': Overall_SEN_ls,\n",
    "    'Overall_SPE_ls_eth1': Overall_SPE_ls_eth1,\n",
    "    'Overall_SPE_ls_eth0': Overall_SPE_ls_eth0,\n",
    "    'Overall_SPE_ls': Overall_SPE_ls,\n",
    "    'Overall_MCC_ls_eth1': Overall_MCC_ls_eth1,\n",
    "    'Overall_MCC_ls_eth0': Overall_MCC_ls_eth0,\n",
    "    'Overall_MCC_ls': Overall_MCC_ls,\n",
    "    'Overall_AUC_ls_eth1': Overall_AUC_ls_eth1,\n",
    "    'Overall_AUC_ls_eth0': Overall_AUC_ls_eth0,\n",
    "    'Overall_AUC_ls': Overall_AUC_ls,\n",
    "    'Overall_AP_ls_eth1': Overall_AP_ls_eth1,\n",
    "    'Overall_AP_ls_eth0': Overall_AP_ls_eth0,\n",
    "    'Overall_AP_ls': Overall_AP_ls,\n",
    "    'No_Skill_ls_eth1': No_Skill_ls_eth1,\n",
    "    'No_Skill_ls_eth0': No_Skill_ls_eth0,\n",
    "    'No_Skill_ls': No_Skill_ls,\n",
    "    'tprs_ls_eth1': tprs_ls_eth1,\n",
    "    'tprs_ls_eth0': tprs_ls_eth0,\n",
    "    'tprs_ls': tprs_ls,\n",
    "    'aucs_ls_eth1': aucs_ls_eth1,\n",
    "    'aucs_ls_eth0': aucs_ls_eth0,\n",
    "    'aucs_ls': aucs_ls,\n",
    "    'mean_fpr_ls_eth1': mean_fpr_ls_eth1,\n",
    "    'mean_fpr_ls_eth0': mean_fpr_ls_eth0,\n",
    "    'mean_fpr_ls': mean_fpr_ls,\n",
    "    'prs_ls_eth1': prs_ls_eth1,\n",
    "    'prs_ls_eth0': prs_ls_eth0,\n",
    "    'prs_ls': prs_ls,\n",
    "    'ap_ls_eth1': ap_ls_eth1,\n",
    "    'ap_ls_eth0': ap_ls_eth0,\n",
    "    'ap_ls': ap_ls,\n",
    "    'mean_recall_ls_eth1': mean_recall_ls_eth1,\n",
    "    'mean_recall_ls_eth0': mean_recall_ls_eth0,\n",
    "    'mean_recall_ls': mean_recall_ls,\n",
    "    'cal_prob_ls_eth1': cal_prob_ls_eth1,\n",
    "    'cal_prob_ls_eth0': cal_prob_ls_eth0,\n",
    "    'cal_prob_ls': cal_prob_ls,\n",
    "    'cal_label_ls_eth1': cal_label_ls_eth1,\n",
    "    'cal_label_ls_eth0': cal_label_ls_eth0,\n",
    "    'cal_label_ls': cal_label_ls,\n",
    "    'Total_attribute': Total_attribute,\n",
    "    'Total_features': Total_features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM WITH BALANCED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overall_ACC_ls_eth1 = []\n",
    "Overall_ACC_ls_eth0 = []\n",
    "Overall_ACC_ls = []\n",
    "Overall_PRC_ls_eth1 = []\n",
    "Overall_PRC_ls_eth0 = []\n",
    "Overall_PRC_ls = []\n",
    "Overall_REC_ls_eth1 = []\n",
    "Overall_REC_ls_eth0 = []\n",
    "Overall_REC_ls = []\n",
    "Overall_F1M_ls_eth1 = []\n",
    "Overall_F1M_ls_eth0 = []\n",
    "Overall_F1M_ls = []\n",
    "Overall_PPV_ls_eth1 = []\n",
    "Overall_PPV_ls_eth0 = []\n",
    "Overall_PPV_ls = []\n",
    "Overall_NPV_ls_eth1 = []\n",
    "Overall_NPV_ls_eth0 = []\n",
    "Overall_NPV_ls = []\n",
    "Overall_SEN_ls_eth1 = []\n",
    "Overall_SEN_ls_eth0 = []\n",
    "Overall_SEN_ls = []\n",
    "Overall_SPE_ls_eth1 = []\n",
    "Overall_SPE_ls_eth0 = []\n",
    "Overall_SPE_ls = []\n",
    "Overall_MCC_ls_eth1 = []\n",
    "Overall_MCC_ls_eth0 = []\n",
    "Overall_MCC_ls = []\n",
    "Overall_AUC_ls_eth1 = []\n",
    "Overall_AUC_ls_eth0 = []\n",
    "Overall_AUC_ls = []\n",
    "Overall_AP_ls_eth1 = []\n",
    "Overall_AP_ls_eth0 = []\n",
    "Overall_AP_ls = []\n",
    "No_Skill_ls_eth1 = []\n",
    "No_Skill_ls_eth0 = []\n",
    "No_Skill_ls = []\n",
    "tprs_ls_eth1 = []\n",
    "tprs_ls_eth0 = []\n",
    "tprs_ls = []\n",
    "aucs_ls_eth1 = []\n",
    "aucs_ls_eth0 = []\n",
    "aucs_ls = []\n",
    "mean_fpr_ls_eth1 = np.linspace(0, 1, 100)\n",
    "mean_fpr_ls_eth0 = np.linspace(0, 1, 100)\n",
    "mean_fpr_ls = np.linspace(0, 1, 100)\n",
    "prs_ls_eth1 = []\n",
    "prs_ls_eth0 = []\n",
    "prs_ls = []\n",
    "ap_ls_eth1 = []\n",
    "ap_ls_eth0 = []\n",
    "ap_ls = []\n",
    "mean_recall_ls_eth1 = np.linspace(0, 1, 100)\n",
    "mean_recall_ls_eth0 = np.linspace(0, 1, 100)\n",
    "mean_recall_ls = np.linspace(0, 1, 100)\n",
    "cal_prob_ls_eth1 = []\n",
    "cal_prob_ls_eth0 = []\n",
    "cal_prob_ls = []\n",
    "cal_label_ls_eth1 = []\n",
    "cal_label_ls_eth0 = []\n",
    "cal_label_ls = []\n",
    "Total_attribute = []\n",
    "\n",
    "Total_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Train with balanced data\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=3, random_state=i*5, shuffle=True)\n",
    "\n",
    "    for train_index, test_index in skf.split(indexing_real.patient_id, indexing_real.stratify_col):\n",
    "\n",
    "\n",
    "        train = indexing_real.iloc[train_index]\n",
    "\n",
    "        test  = indexing_real.iloc[test_index]\n",
    "        \n",
    "        # Add fake data indeces\n",
    "        # Count unique IDs for each ethnicity in the training set\n",
    "        train_eth0_count = train[train['ethnicity'] == 0]['patient_id'].nunique()\n",
    "        train_eth1_count = train[train['ethnicity'] == 1]['patient_id'].nunique()\n",
    "        \n",
    "        # Determine how many fake samples to add\n",
    "        n_fake_to_add = train_eth0_count - train_eth1_count\n",
    "           \n",
    "        # If we have more fake samples than needed, randomly select subset\n",
    "        if len(indexing_fake) > n_fake_to_add:\n",
    "            fake_to_add = indexing_fake.sample(n=n_fake_to_add, random_state=42)\n",
    "        else:\n",
    "            fake_to_add = indexing_fake\n",
    "        \n",
    "        # Concatenate fake samples to the training set\n",
    "        train = pd.concat([train, fake_to_add], ignore_index=True)\n",
    "\n",
    "\n",
    "        train, valid = train_test_split(train, stratify= train.label, test_size= 0.20, random_state= 123)\n",
    "\n",
    "\n",
    "        lactate_train_id = train.patient_id.unique()\n",
    "        \n",
    "        lactate_valid_id = valid.patient_id.unique()\n",
    "\n",
    "        lactate_test_id  = test.patient_id.unique()\n",
    "\n",
    "\n",
    "        train_df = merged_data[merged_data['patient_id'].isin(lactate_train_id)]\n",
    "        \n",
    "        valid_df = merged_data[merged_data['patient_id'].isin(lactate_valid_id)]\n",
    "\n",
    "        test_df  = merged_data[merged_data['patient_id'].isin(lactate_test_id)]\n",
    "\n",
    "        print(\"Ethnicity counts after balancing:\")\n",
    "        print(train_df.groupby('patient_id')['ethnicity'].first().value_counts())\n",
    "\n",
    "        train_df_normilized, valid_df_normilized, test_df_normalized = Normalizing(train_df, valid_df, test_df)\n",
    "\n",
    "\n",
    "        X_train, y_train, ethnicity_train = Data_Prepare(train_df_normilized)\n",
    "        \n",
    "        X_valid, y_valid, ethnicity_valid = Data_Prepare(valid_df_normilized)\n",
    "\n",
    "        X_test , y_test, ethnicity_test  = Data_Prepare(test_df_normalized)\n",
    "\n",
    "\n",
    "        X_data_train = np.array(X_train)\n",
    "        \n",
    "        X_data_valid = np.array(X_valid)\n",
    "        \n",
    "        X_data_test  = np.array(X_test)\n",
    "        \n",
    "        \n",
    "        y_train = np.array(y_train)\n",
    "        \n",
    "        y_valid = np.array(y_valid)\n",
    "        \n",
    "        y_test  = np.array(y_test)\n",
    "\n",
    "\n",
    "        ethnicity_test = np.array(ethnicity_test)\n",
    "\n",
    "\n",
    "        X_len_train = np.full(len(y_train), 14)\n",
    "        X_len_valid = np.full(len(y_valid), 14)\n",
    "        X_len_test = np.full(len(y_test), 14)\n",
    "\n",
    "\n",
    "        train_data = LactateData(X_data_train, y_train, X_len_train)\n",
    "        valid_data = LactateData(X_data_valid, y_valid, X_len_valid)\n",
    "        test_data  = LactateData(X_data_test , y_test , X_len_test)\n",
    "        \n",
    "        \n",
    "        train_class_sample_count = torch.tensor([(torch.tensor(y_train) == t).sum() for t in torch.unique(torch.tensor(y_train), sorted=True)])\n",
    "        train_weight  = 1 / train_class_sample_count.float()\n",
    "        train_samples_weight = torch.tensor([train_weight[i] for i in torch.tensor(y_train).long()])\n",
    "        train_sampler = WeightedRandomSampler(train_samples_weight, len(train_samples_weight))\n",
    "\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False, sampler=train_sampler, **kwargs)\n",
    "        valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        test_loader  = DataLoader(test_data , batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        \n",
    "        \n",
    "        model = build_model(input_size, hidden_size, number_layers, number_classes)\n",
    "        \n",
    "        model.apply(weight_init)\n",
    "        \n",
    "        \n",
    "        class_weight = torch.Tensor([0.65, 0.35])\n",
    "        class_weight = class_weight.to(device)\n",
    "        criterion    = nn.CrossEntropyLoss(weight= class_weight)\n",
    "        \n",
    "        l1_crit   = nn.L1Loss(size_average=False)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        ##########################################################################\n",
    "        \n",
    "        # Initialize early stopping\n",
    "        early_stopping = EarlyStopping(patience=10, verbose=False)\n",
    "\n",
    "        train_losses = []\n",
    "        valid_losses = []  # Added to track validation losses\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0\n",
    "            model.train()\n",
    "\n",
    "            for x, y, l in train_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                l = l.to(device)\n",
    "                y = y.long()\n",
    "\n",
    "                seq_lengths, perm_idx = l.sort(0, descending=True)\n",
    "                x = x[perm_idx]\n",
    "                y = y[perm_idx]\n",
    "\n",
    "                outputs = model(x, seq_lengths)\n",
    "                entropy_loss = criterion(outputs, y)\n",
    "\n",
    "                l1_loss_ = 0\n",
    "                for param in model.lstm.parameters():\n",
    "                    l1_loss_ += l1_crit(param, target=torch.zeros_like(param))\n",
    "                factor_1 = 0.0006\n",
    "                l1_loss = factor_1 * l1_loss_\n",
    "\n",
    "                loss = entropy_loss + l1_loss \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()   \n",
    "\n",
    "                train_loss += loss.item()   \n",
    "            \n",
    "            train_losses.append(train_loss/len(train_loader))\n",
    "\n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for x, y, l in valid_loader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    l = l.to(device)\n",
    "                    y = y.long()\n",
    "\n",
    "                    seq_lengths, perm_idx = l.sort(0, descending=True)\n",
    "                    x = x[perm_idx]\n",
    "                    y = y[perm_idx]\n",
    "\n",
    "                    outputs = model(x, seq_lengths)\n",
    "                    loss = criterion(outputs, y)  # Using only entropy loss for validation\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "            valid_loss = valid_loss / len(valid_loader)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            #print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "            # Early stopping\n",
    "            early_stopping(valid_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        print(f\"TRAIN ENDED (epochs = {epoch+1})\")\n",
    "            \n",
    "        ##########################################################################\n",
    "        \n",
    "        temp_model = ModelWithTemperature(model)\n",
    "        \n",
    "        temp_model.set_temperature(valid_loader)\n",
    "        \n",
    "        temp_model.train_temprature(valid_loader)    \n",
    "       \n",
    "        ##########################################################################\n",
    "        \n",
    "        test_losses = []\n",
    "            \n",
    "        temp_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            total = 0\n",
    "            \n",
    "            test_loss = 0\n",
    "\n",
    "            test_labels = []\n",
    "\n",
    "            test_probs  = []\n",
    "\n",
    "            for x, y, l in test_loader:\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                l = l.to(device)\n",
    "\n",
    "                y = y.long()\n",
    "\n",
    "\n",
    "                ###\n",
    "                seq_lengths, perm_idx = l.sort(0, descending=True)\n",
    "\n",
    "                x = x[perm_idx]\n",
    "                y = y[perm_idx]\n",
    "                ###\n",
    "\n",
    "\n",
    "                output = temp_model(x, seq_lengths)\n",
    "                \n",
    "                entropy_loss = criterion(output, y)\n",
    "                \n",
    "                \n",
    "                l1_loss_ = 0\n",
    "                \n",
    "                for param in model.lstm.parameters():\n",
    "            \n",
    "                    l1_loss_ += l1_crit(param, target=torch.zeros_like(param))\n",
    "            \n",
    "                factor = 0.0006\n",
    "        \n",
    "                l1_loss = factor * l1_loss_\n",
    "\n",
    "                                \n",
    "                loss = entropy_loss + l1_loss \n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                \n",
    "\n",
    "                outputs = nn.Softmax()(output)\n",
    "\n",
    "                prediction = outputs.detach().cpu().numpy()\n",
    "\n",
    "                prediction = prediction[:,1]\n",
    "\n",
    "                test_labels.append(y.detach().cpu().numpy())\n",
    "\n",
    "                test_probs.append(prediction)\n",
    "                \n",
    "                \n",
    "            test_losses.append(test_loss/len(test_loader))\n",
    "        \n",
    "            #print(\"Test loss: {:0.4f} \".format(test_loss/len(test_loader)))\n",
    "        \n",
    "            #print(\"TEST ENDED\")\n",
    "            \n",
    "            \n",
    "            test_labels = [l for labels in test_labels for l in labels]\n",
    "        \n",
    "            test_probs  = [p for probs  in test_probs  for p in probs]\n",
    "                        \n",
    "        ##########################################################################\n",
    "            \n",
    "\n",
    "        pred_probs_eth0 = np.array(test_probs)[ethnicity_test == 0]\n",
    "        true_labels_eth0 = np.array(test_labels)[ethnicity_test == 0]\n",
    "\n",
    "        pred_probs_eth1 = np.array(test_probs)[ethnicity_test == 1]\n",
    "        true_labels_eth1 = np.array(test_labels)[ethnicity_test == 1]\n",
    "\n",
    "                # For overall (without ethnicity specification)\n",
    "        cal_label_ls.append(test_labels)\n",
    "        cal_prob_ls.append(test_probs)\n",
    "\n",
    "        TN, FP, FN, TP = confusion_matrix(test_labels, np.array(test_probs).round()).ravel()\n",
    "\n",
    "        ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        PRC = (TP)/(TP+FP)\n",
    "        REC = (TP)/(TP+FN)\n",
    "        F1M = (2*PRC*REC)/(PRC+REC)\n",
    "        PPV = TP/(TP+FP)\n",
    "        NPV = TN/(TN+FN)\n",
    "        SEN = (TP)/(TP+FN)\n",
    "        SPE = (TN)/(TN+FP)\n",
    "        MCC = matthews_corrcoef(test_labels, np.array(test_probs).round())\n",
    "        AUC = roc_auc_score(test_labels, test_probs)\n",
    "        AP = average_precision_score(test_labels, test_probs)\n",
    "\n",
    "        Overall_ACC_ls.append(ACC)\n",
    "        Overall_PRC_ls.append(PRC)\n",
    "        Overall_REC_ls.append(REC)\n",
    "        Overall_F1M_ls.append(F1M)\n",
    "        Overall_PPV_ls.append(PPV)\n",
    "        Overall_NPV_ls.append(NPV)\n",
    "        Overall_SEN_ls.append(SEN)\n",
    "        Overall_SPE_ls.append(SPE)\n",
    "        Overall_MCC_ls.append(MCC)\n",
    "        Overall_AUC_ls.append(AUC)\n",
    "        Overall_AP_ls.append(AP)\n",
    "\n",
    "        fpr_lstm, tpr_lstm, thresholds_lstm = roc_curve(test_labels, test_probs)\n",
    "        tprs_ls.append(interp(mean_fpr_ls, fpr_lstm, tpr_lstm))\n",
    "        tprs_ls[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr_lstm, tpr_lstm)\n",
    "        aucs_ls.append(roc_auc)\n",
    "\n",
    "        no_skill = len([lab for lab in test_labels if lab == 1]) / len(test_labels)\n",
    "        No_Skill_ls.append(no_skill)\n",
    "\n",
    "        precision_lstm, recall_lstm, threshold_lstm = precision_recall_curve(test_labels, test_probs)\n",
    "        prs_ls.append(interp(mean_recall_ls, precision_lstm, recall_lstm))\n",
    "        pr_ap = auc(recall_lstm, precision_lstm)\n",
    "        ap_ls.append(pr_ap)\n",
    "\n",
    "\n",
    "        # For eth1\n",
    "        cal_label_ls_eth1.append(true_labels_eth1)\n",
    "        cal_prob_ls_eth1.append(pred_probs_eth1)\n",
    "\n",
    "        TN_eth1, FP_eth1, FN_eth1, TP_eth1 = confusion_matrix(true_labels_eth1, pred_probs_eth1.round()).ravel()\n",
    "\n",
    "        ACC_eth1 = (TP_eth1+TN_eth1)/(TP_eth1+FP_eth1+FN_eth1+TN_eth1)\n",
    "        PRC_eth1 = (TP_eth1)/(TP_eth1+FP_eth1)\n",
    "        REC_eth1 = (TP_eth1)/(TP_eth1+FN_eth1)\n",
    "        F1M_eth1 = (2*PRC_eth1*REC_eth1)/(PRC_eth1+REC_eth1)\n",
    "        PPV_eth1 = TP_eth1/(TP_eth1+FP_eth1)\n",
    "        NPV_eth1 = TN_eth1/(TN_eth1+FN_eth1)\n",
    "        SEN_eth1 = (TP_eth1)/(TP_eth1+FN_eth1)\n",
    "        SPE_eth1 = (TN_eth1)/(TN_eth1+FP_eth1)\n",
    "        MCC_eth1 = matthews_corrcoef(true_labels_eth1, pred_probs_eth1.round())\n",
    "        AUC_eth1 = roc_auc_score(true_labels_eth1, pred_probs_eth1)\n",
    "        AP_eth1 = average_precision_score(true_labels_eth1, pred_probs_eth1)\n",
    "\n",
    "        Overall_ACC_ls_eth1.append(ACC_eth1)\n",
    "        Overall_PRC_ls_eth1.append(PRC_eth1)\n",
    "        Overall_REC_ls_eth1.append(REC_eth1)\n",
    "        Overall_F1M_ls_eth1.append(F1M_eth1)\n",
    "        Overall_PPV_ls_eth1.append(PPV_eth1)\n",
    "        Overall_NPV_ls_eth1.append(NPV_eth1)\n",
    "        Overall_SEN_ls_eth1.append(SEN_eth1)\n",
    "        Overall_SPE_ls_eth1.append(SPE_eth1)\n",
    "        Overall_MCC_ls_eth1.append(MCC_eth1)\n",
    "        Overall_AUC_ls_eth1.append(AUC_eth1)\n",
    "        Overall_AP_ls_eth1.append(AP_eth1)\n",
    "\n",
    "        fpr_lstm_eth1, tpr_lstm_eth1, thresholds_lstm_eth1 = roc_curve(true_labels_eth1, pred_probs_eth1)\n",
    "        tprs_ls_eth1.append(interp(mean_fpr_ls_eth1, fpr_lstm_eth1, tpr_lstm_eth1))\n",
    "        tprs_ls_eth1[-1][0] = 0.0\n",
    "        roc_auc_eth1 = auc(fpr_lstm_eth1, tpr_lstm_eth1)\n",
    "        aucs_ls_eth1.append(roc_auc_eth1)\n",
    "\n",
    "        no_skill_eth1 = len([lab for lab in true_labels_eth1 if lab == 1]) / len(true_labels_eth1)\n",
    "        No_Skill_ls_eth1.append(no_skill_eth1)\n",
    "\n",
    "        precision_lstm_eth1, recall_lstm_eth1, threshold_lstm_eth1 = precision_recall_curve(true_labels_eth1, pred_probs_eth1)\n",
    "        prs_ls_eth1.append(interp(mean_recall_ls_eth1, precision_lstm_eth1, recall_lstm_eth1))\n",
    "        pr_ap_eth1 = auc(recall_lstm_eth1, precision_lstm_eth1)\n",
    "        ap_ls_eth1.append(pr_ap_eth1)\n",
    "\n",
    "        # For eth0\n",
    "        cal_label_ls_eth0.append(true_labels_eth0)\n",
    "        cal_prob_ls_eth0.append(pred_probs_eth0)\n",
    "\n",
    "        TN_eth0, FP_eth0, FN_eth0, TP_eth0 = confusion_matrix(true_labels_eth0, pred_probs_eth0.round()).ravel()\n",
    "\n",
    "        ACC_eth0 = (TP_eth0+TN_eth0)/(TP_eth0+FP_eth0+FN_eth0+TN_eth0)\n",
    "        PRC_eth0 = (TP_eth0)/(TP_eth0+FP_eth0)\n",
    "        REC_eth0 = (TP_eth0)/(TP_eth0+FN_eth0)\n",
    "        F1M_eth0 = (2*PRC_eth0*REC_eth0)/(PRC_eth0+REC_eth0)\n",
    "        PPV_eth0 = TP_eth0/(TP_eth0+FP_eth0)\n",
    "        NPV_eth0 = TN_eth0/(TN_eth0+FN_eth0)\n",
    "        SEN_eth0 = (TP_eth0)/(TP_eth0+FN_eth0)\n",
    "        SPE_eth0 = (TN_eth0)/(TN_eth0+FP_eth0)\n",
    "        MCC_eth0 = matthews_corrcoef(true_labels_eth0, pred_probs_eth0.round())\n",
    "        AUC_eth0 = roc_auc_score(true_labels_eth0, pred_probs_eth0)\n",
    "        AP_eth0 = average_precision_score(true_labels_eth0, pred_probs_eth0)\n",
    "\n",
    "        Overall_ACC_ls_eth0.append(ACC_eth0)\n",
    "        Overall_PRC_ls_eth0.append(PRC_eth0)\n",
    "        Overall_REC_ls_eth0.append(REC_eth0)\n",
    "        Overall_F1M_ls_eth0.append(F1M_eth0)\n",
    "        Overall_PPV_ls_eth0.append(PPV_eth0)\n",
    "        Overall_NPV_ls_eth0.append(NPV_eth0)\n",
    "        Overall_SEN_ls_eth0.append(SEN_eth0)\n",
    "        Overall_SPE_ls_eth0.append(SPE_eth0)\n",
    "        Overall_MCC_ls_eth0.append(MCC_eth0)\n",
    "        Overall_AUC_ls_eth0.append(AUC_eth0)\n",
    "        Overall_AP_ls_eth0.append(AP_eth0)\n",
    "\n",
    "        fpr_lstm_eth0, tpr_lstm_eth0, thresholds_lstm_eth0 = roc_curve(true_labels_eth0, pred_probs_eth0)\n",
    "        tprs_ls_eth0.append(interp(mean_fpr_ls_eth0, fpr_lstm_eth0, tpr_lstm_eth0))\n",
    "        tprs_ls_eth0[-1][0] = 0.0\n",
    "        roc_auc_eth0 = auc(fpr_lstm_eth0, tpr_lstm_eth0)\n",
    "        aucs_ls_eth0.append(roc_auc_eth0)\n",
    "\n",
    "        no_skill_eth0 = len([lab for lab in true_labels_eth0 if lab == 1]) / len(true_labels_eth0)\n",
    "        No_Skill_ls_eth0.append(no_skill_eth0)\n",
    "\n",
    "        precision_lstm_eth0, recall_lstm_eth0, threshold_lstm_eth0 = precision_recall_curve(true_labels_eth0, pred_probs_eth0)\n",
    "        prs_ls_eth0.append(interp(mean_recall_ls_eth0, precision_lstm_eth0, recall_lstm_eth0))\n",
    "        pr_ap_eth0 = auc(recall_lstm_eth0, precision_lstm_eth0)\n",
    "        ap_ls_eth0.append(pr_ap_eth0)\n",
    "        \n",
    "        ##########################################################################\n",
    "            \n",
    "        IG = IntegratedGradients(temp_model)\n",
    "        \n",
    "        temp_model.train()\n",
    "    \n",
    "        for x, y, l in test_loader:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            l = l.to(device)\n",
    "\n",
    "            y = y.long()\n",
    "\n",
    "\n",
    "            ###\n",
    "            seq_lengths, perm_idx = l.sort(0, descending=True)\n",
    "\n",
    "            x = x[perm_idx]\n",
    "            y = y[perm_idx]\n",
    "            ###\n",
    "\n",
    "\n",
    "            attribute = IG.attribute(x, additional_forward_args=seq_lengths, target=0)\n",
    "\n",
    "            Total_attribute.append(attribute.detach().cpu().numpy())\n",
    "            \n",
    "        \n",
    "        Total_features.append(X_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ethnicity: 0\")\n",
    "\n",
    "print(f\"Mean ACC: {np.mean(Overall_ACC_ls_eth0):.4f}\")\n",
    "print(f\"Mean PRC: {np.mean(Overall_PRC_ls_eth0):.4f}\")\n",
    "print(f\"Mean REC: {np.mean(Overall_REC_ls_eth0):.4f}\")\n",
    "print(f\"Mean F1M: {np.mean(Overall_F1M_ls_eth0):.4f}\")\n",
    "print(f\"Mean PPV: {np.mean(Overall_PPV_ls_eth0):.4f}\")\n",
    "print(f\"Mean NPV: {np.mean(Overall_NPV_ls_eth0):.4f}\")\n",
    "print(f\"Mean SEN: {np.mean(Overall_SEN_ls_eth0):.4f}\")\n",
    "print(f\"Mean SPE: {np.mean(Overall_SPE_ls_eth0):.4f}\")\n",
    "print(f\"Mean MCC: {np.mean(Overall_MCC_ls_eth0):.4f}\")\n",
    "print(f\"Mean AUC: {np.mean(Overall_AUC_ls_eth0):.4f}\")\n",
    "print(f\"Mean AP: {np.mean(Overall_AP_ls_eth0):.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Ethnicity: 1\")\n",
    "\n",
    "print(f\"Mean ACC: {np.mean(Overall_ACC_ls_eth1):.4f}\")\n",
    "print(f\"Mean PRC: {np.mean(Overall_PRC_ls_eth1):.4f}\")\n",
    "print(f\"Mean REC: {np.mean(Overall_REC_ls_eth1):.4f}\")\n",
    "print(f\"Mean F1M: {np.mean(Overall_F1M_ls_eth1):.4f}\")\n",
    "print(f\"Mean PPV: {np.mean(Overall_PPV_ls_eth1):.4f}\")\n",
    "print(f\"Mean NPV: {np.mean(Overall_NPV_ls_eth1):.4f}\")\n",
    "print(f\"Mean SEN: {np.mean(Overall_SEN_ls_eth1):.4f}\")\n",
    "print(f\"Mean SPE: {np.mean(Overall_SPE_ls_eth1):.4f}\")\n",
    "print(f\"Mean MCC: {np.mean(Overall_MCC_ls_eth1):.4f}\")\n",
    "print(f\"Mean AUC: {np.mean(Overall_AUC_ls_eth1):.4f}\")\n",
    "print(f\"Mean AP: {np.mean(Overall_AP_ls_eth1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ethnicity: 0\")\n",
    "\n",
    "print(f\"Std ACC: {np.std(Overall_ACC_ls_eth0):.4f}\")\n",
    "print(f\"Std PRC: {np.std(Overall_PRC_ls_eth0):.4f}\")\n",
    "print(f\"Std REC: {np.std(Overall_REC_ls_eth0):.4f}\")\n",
    "print(f\"Std F1M: {np.std(Overall_F1M_ls_eth0):.4f}\")\n",
    "print(f\"Std PPV: {np.std(Overall_PPV_ls_eth0):.4f}\")\n",
    "print(f\"Std NPV: {np.std(Overall_NPV_ls_eth0):.4f}\")\n",
    "print(f\"Std SEN: {np.std(Overall_SEN_ls_eth0):.4f}\")\n",
    "print(f\"Std SPE: {np.std(Overall_SPE_ls_eth0):.4f}\")\n",
    "print(f\"Std MCC: {np.std(Overall_MCC_ls_eth0):.4f}\")\n",
    "print(f\"Std AUC: {np.std(Overall_AUC_ls_eth0):.4f}\")\n",
    "print(f\"Std AP: {np.std(Overall_AP_ls_eth0):.4f}\")\n",
    "\n",
    "print(\"\\n\")  # Add a blank line for separation\n",
    "\n",
    "print(\"Ethnicity: 1\")\n",
    "\n",
    "print(f\"Std ACC: {np.std(Overall_ACC_ls_eth1):.4f}\")\n",
    "print(f\"Std PRC: {np.std(Overall_PRC_ls_eth1):.4f}\")\n",
    "print(f\"Std REC: {np.std(Overall_REC_ls_eth1):.4f}\")\n",
    "print(f\"Std F1M: {np.std(Overall_F1M_ls_eth1):.4f}\")\n",
    "print(f\"Std PPV: {np.std(Overall_PPV_ls_eth1):.4f}\")\n",
    "print(f\"Std NPV: {np.std(Overall_NPV_ls_eth1):.4f}\")\n",
    "print(f\"Std SEN: {np.std(Overall_SEN_ls_eth1):.4f}\")\n",
    "print(f\"Std SPE: {np.std(Overall_SPE_ls_eth1):.4f}\")\n",
    "print(f\"Std MCC: {np.std(Overall_MCC_ls_eth1):.4f}\")\n",
    "print(f\"Std AUC: {np.std(Overall_AUC_ls_eth1):.4f}\")\n",
    "print(f\"Std AP: {np.std(Overall_AP_ls_eth1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['ACC', 'PRC', 'REC', 'F1M', 'PPV', 'NPV', 'SEN', 'SPE', 'MCC', 'AUC', 'AP']\n",
    "\n",
    "print(\"Results for balanced data:\\n\")\n",
    "\n",
    "print(f\"{'Metric':<7}{'Overall (Mean ± Std)':<30}{'Ethnicity 0 (Mean ± Std)':<30}{'Ethnicity 1 (Mean ± Std)':<30}\")\n",
    "\n",
    "print('-' * 96)\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "    eth0_mean = np.nanmean(globals()[f'Overall_{metric}_ls_eth0'])\n",
    "    eth0_std = np.nanstd(globals()[f'Overall_{metric}_ls_eth0'])\n",
    "    eth1_mean = np.nanmean(globals()[f'Overall_{metric}_ls_eth1'])\n",
    "    eth1_std = np.nanstd(globals()[f'Overall_{metric}_ls_eth1'])\n",
    "\n",
    "    full_mean = np.nanmean(globals()[f'Overall_{metric}_ls'])\n",
    "    full_std = np.nanstd(globals()[f'Overall_{metric}_ls'])\n",
    "    \n",
    "    print(f\"{metric:<7}{full_mean:.4f} ± {full_std:<21.4f}{eth0_mean:.4f} ± {eth0_std:<21.4f}{eth1_mean:.4f} ± {eth1_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "metrics = ['ACC', 'PRC', 'REC', 'F1M', 'PPV', 'NPV', 'SEN', 'SPE', 'MCC', 'AUC', 'AP']\n",
    "\n",
    "print(\"Results for balanced data:\\n\")\n",
    "\n",
    "print(f\"{'Metric':<7}{'Full data (Mean [95% CI])':<35}{'Ethnicity 0 (Mean [95% CI])':<35}{'Ethnicity 1 (Mean [95% CI])':<35}\")\n",
    "\n",
    "print('-' * 77)\n",
    "\n",
    "for metric in metrics:\n",
    "    eth0_data = globals()[f'Overall_{metric}_ls_eth0']\n",
    "    eth1_data = globals()[f'Overall_{metric}_ls_eth1']\n",
    "    full_data = globals()[f'Overall_{metric}_ls']\n",
    "    \n",
    "    eth0_mean = np.nanmean(eth0_data)\n",
    "    eth1_mean = np.nanmean(eth1_data)\n",
    "    full_mean = np.nanmean(full_data)\n",
    "    \n",
    "    eth0_sem = stats.sem(eth0_data, nan_policy='omit')\n",
    "    eth1_sem = stats.sem(eth1_data, nan_policy='omit')\n",
    "    full_sem = stats.sem(full_data, nan_policy='omit')\n",
    "    \n",
    "    eth0_ci = eth0_mean - 1.96*eth0_sem, eth0_mean + 1.96*eth0_sem\n",
    "    eth1_ci = eth1_mean - 1.96*eth1_sem, eth1_mean + 1.96*eth1_sem\n",
    "    full_ci = full_mean - 1.96*full_sem, full_mean + 1.96*full_sem\n",
    "    \n",
    "    print(f\"{metric:<7}{full_mean:.4f} [{full_ci[0]:.4f}, {full_ci[1]:.4f}]{' '*5}{eth0_mean:.4f} [{eth0_ci[0]:.4f}, {eth0_ci[1]:.4f}]{' '*5}{eth1_mean:.4f} [{eth1_ci[0]:.4f}, {eth1_ci[1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for Ethnicity 0\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='green', label='Random', alpha=.8)\n",
    "\n",
    "mean_tpr_ls_eth0 = np.mean(tprs_ls_eth0, axis=0)\n",
    "mean_tpr_ls_eth0[-1] = 1.0\n",
    "mean_auc_ls_eth0 = auc(mean_fpr_ls_eth0, mean_tpr_ls_eth0)\n",
    "std_auc_ls_eth0 = np.std(aucs_ls_eth0)\n",
    "plt.plot(mean_fpr_ls_eth0, mean_tpr_ls_eth0, color='r', \n",
    "         label=r'LSTM Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_ls_eth0, std_auc_ls_eth0), \n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr_ls_eth0 = np.std(tprs_ls_eth0, axis=0)\n",
    "tprs_upper_ls_eth0 = np.minimum(mean_tpr_ls_eth0 + std_tpr_ls_eth0, 1)\n",
    "tprs_lower_ls_eth0 = np.maximum(mean_tpr_ls_eth0 - std_tpr_ls_eth0, 0)\n",
    "plt.fill_between(mean_fpr_ls_eth0, tprs_lower_ls_eth0, tprs_upper_ls_eth0, color='grey', alpha=.2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Ethnicity 0')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Plot for Ethnicity 1\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='green', label='Random', alpha=.8)\n",
    "\n",
    "mean_tpr_ls_eth1 = np.mean(tprs_ls_eth1, axis=0)\n",
    "mean_tpr_ls_eth1[-1] = 1.0\n",
    "mean_auc_ls_eth1 = auc(mean_fpr_ls_eth1, mean_tpr_ls_eth1)\n",
    "std_auc_ls_eth1 = np.std(aucs_ls_eth1)\n",
    "plt.plot(mean_fpr_ls_eth1, mean_tpr_ls_eth1, color='b', \n",
    "         label=r'LSTM Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_ls_eth1, std_auc_ls_eth1), \n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr_ls_eth1 = np.std(tprs_ls_eth1, axis=0)\n",
    "tprs_upper_ls_eth1 = np.minimum(mean_tpr_ls_eth1 + std_tpr_ls_eth1, 1)\n",
    "tprs_lower_ls_eth1 = np.maximum(mean_tpr_ls_eth1 - std_tpr_ls_eth1, 0)\n",
    "plt.fill_between(mean_fpr_ls_eth1, tprs_lower_ls_eth1, tprs_upper_ls_eth1, color='grey', alpha=.2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Ethnicity 1')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for Ethnicity 0\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([0, 1], [np.mean(No_Skill_ls_eth0), np.mean(No_Skill_ls_eth0)], linestyle='--', lw=2, color='green', label='Random', alpha=.8)\n",
    "\n",
    "mean_prs_ls_eth0 = np.mean(prs_ls_eth0, axis=0)\n",
    "mean_ap_ls_eth0 = auc(mean_recall_ls_eth0, mean_prs_ls_eth0)\n",
    "std_ap_ls_eth0 = np.std(ap_ls_eth0)\n",
    "plt.plot(mean_recall_ls_eth0, mean_prs_ls_eth0, color='r', \n",
    "         label=r'Mean (AP = %0.3f $\\pm$ %0.2f)' % (mean_ap_ls_eth0, std_ap_ls_eth0), \n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_prs_ls_eth0 = np.std(prs_ls_eth0, axis=0)\n",
    "prs_upper_ls_eth0 = np.minimum(mean_prs_ls_eth0 + std_prs_ls_eth0, 1)\n",
    "prs_lower_ls_eth0 = np.maximum(mean_prs_ls_eth0 - std_prs_ls_eth0, 0)\n",
    "plt.fill_between(mean_recall_ls_eth0, prs_lower_ls_eth0, prs_upper_ls_eth0, color='grey', alpha=.2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve - Ethnicity 0')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot for Ethnicity 1\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([0, 1], [np.mean(No_Skill_ls_eth1), np.mean(No_Skill_ls_eth1)], linestyle='--', lw=2, color='green', label='Random', alpha=.8)\n",
    "\n",
    "mean_prs_ls_eth1 = np.mean(prs_ls_eth1, axis=0)\n",
    "mean_ap_ls_eth1 = auc(mean_recall_ls_eth1, mean_prs_ls_eth1)\n",
    "std_ap_ls_eth1 = np.std(ap_ls_eth1)\n",
    "plt.plot(mean_recall_ls_eth1, mean_prs_ls_eth1, color='b', \n",
    "         label=r'Mean (AP = %0.3f $\\pm$ %0.2f)' % (mean_ap_ls_eth1, std_ap_ls_eth1), \n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_prs_ls_eth1 = np.std(prs_ls_eth1, axis=0)\n",
    "prs_upper_ls_eth1 = np.minimum(mean_prs_ls_eth1 + std_prs_ls_eth1, 1)\n",
    "prs_lower_ls_eth1 = np.maximum(mean_prs_ls_eth1 - std_prs_ls_eth1, 0)\n",
    "plt.fill_between(mean_recall_ls_eth1, prs_lower_ls_eth1, prs_upper_ls_eth1, color='grey', alpha=.2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve - Ethnicity 1')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_balanced = {\n",
    "    'Overall_ACC_ls_eth1': Overall_ACC_ls_eth1,\n",
    "    'Overall_ACC_ls_eth0': Overall_ACC_ls_eth0,\n",
    "    'Overall_ACC_ls': Overall_ACC_ls,\n",
    "    'Overall_PRC_ls_eth1': Overall_PRC_ls_eth1,\n",
    "    'Overall_PRC_ls_eth0': Overall_PRC_ls_eth0,\n",
    "    'Overall_PRC_ls': Overall_PRC_ls,\n",
    "    'Overall_REC_ls_eth1': Overall_REC_ls_eth1,\n",
    "    'Overall_REC_ls_eth0': Overall_REC_ls_eth0,\n",
    "    'Overall_REC_ls': Overall_REC_ls,\n",
    "    'Overall_F1M_ls_eth1': Overall_F1M_ls_eth1,\n",
    "    'Overall_F1M_ls_eth0': Overall_F1M_ls_eth0,\n",
    "    'Overall_F1M_ls': Overall_F1M_ls,\n",
    "    'Overall_PPV_ls_eth1': Overall_PPV_ls_eth1,\n",
    "    'Overall_PPV_ls_eth0': Overall_PPV_ls_eth0,\n",
    "    'Overall_PPV_ls': Overall_PPV_ls,\n",
    "    'Overall_NPV_ls_eth1': Overall_NPV_ls_eth1,\n",
    "    'Overall_NPV_ls_eth0': Overall_NPV_ls_eth0,\n",
    "    'Overall_NPV_ls': Overall_NPV_ls,\n",
    "    'Overall_SEN_ls_eth1': Overall_SEN_ls_eth1,\n",
    "    'Overall_SEN_ls_eth0': Overall_SEN_ls_eth0,\n",
    "    'Overall_SEN_ls': Overall_SEN_ls,\n",
    "    'Overall_SPE_ls_eth1': Overall_SPE_ls_eth1,\n",
    "    'Overall_SPE_ls_eth0': Overall_SPE_ls_eth0,\n",
    "    'Overall_SPE_ls': Overall_SPE_ls,\n",
    "    'Overall_MCC_ls_eth1': Overall_MCC_ls_eth1,\n",
    "    'Overall_MCC_ls_eth0': Overall_MCC_ls_eth0,\n",
    "    'Overall_MCC_ls': Overall_MCC_ls,\n",
    "    'Overall_AUC_ls_eth1': Overall_AUC_ls_eth1,\n",
    "    'Overall_AUC_ls_eth0': Overall_AUC_ls_eth0,\n",
    "    'Overall_AUC_ls': Overall_AUC_ls,\n",
    "    'Overall_AP_ls_eth1': Overall_AP_ls_eth1,\n",
    "    'Overall_AP_ls_eth0': Overall_AP_ls_eth0,\n",
    "    'Overall_AP_ls': Overall_AP_ls,\n",
    "    'No_Skill_ls_eth1': No_Skill_ls_eth1,\n",
    "    'No_Skill_ls_eth0': No_Skill_ls_eth0,\n",
    "    'No_Skill_ls': No_Skill_ls,\n",
    "    'tprs_ls_eth1': tprs_ls_eth1,\n",
    "    'tprs_ls_eth0': tprs_ls_eth0,\n",
    "    'tprs_ls': tprs_ls,\n",
    "    'aucs_ls_eth1': aucs_ls_eth1,\n",
    "    'aucs_ls_eth0': aucs_ls_eth0,\n",
    "    'aucs_ls': aucs_ls,\n",
    "    'mean_fpr_ls_eth1': mean_fpr_ls_eth1,\n",
    "    'mean_fpr_ls_eth0': mean_fpr_ls_eth0,\n",
    "    'mean_fpr_ls': mean_fpr_ls,\n",
    "    'prs_ls_eth1': prs_ls_eth1,\n",
    "    'prs_ls_eth0': prs_ls_eth0,\n",
    "    'prs_ls': prs_ls,\n",
    "    'ap_ls_eth1': ap_ls_eth1,\n",
    "    'ap_ls_eth0': ap_ls_eth0,\n",
    "    'ap_ls': ap_ls,\n",
    "    'mean_recall_ls_eth1': mean_recall_ls_eth1,\n",
    "    'mean_recall_ls_eth0': mean_recall_ls_eth0,\n",
    "    'mean_recall_ls': mean_recall_ls,\n",
    "    'cal_prob_ls_eth1': cal_prob_ls_eth1,\n",
    "    'cal_prob_ls_eth0': cal_prob_ls_eth0,\n",
    "    'cal_prob_ls': cal_prob_ls,\n",
    "    'cal_label_ls_eth1': cal_label_ls_eth1,\n",
    "    'cal_label_ls_eth0': cal_label_ls_eth0,\n",
    "    'cal_label_ls': cal_label_ls,\n",
    "    'Total_attribute': Total_attribute,\n",
    "    'Total_features': Total_features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_difference(results, metric):\n",
    "    eth0 = results[f'{metric}_ls_eth0']\n",
    "    eth1 = results[f'{metric}_ls_eth1']\n",
    "    return [e0 - e1 for e1, e0 in zip(eth1, eth0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_compare = ['Overall_ACC', 'Overall_AUC', 'Overall_MCC']\n",
    "\n",
    "for metric in metrics_to_compare:\n",
    "    real_diff = calculate_difference(results_real, metric)\n",
    "    balanced_diff = calculate_difference(results_balanced, metric)\n",
    "    \n",
    "    print(f\"\\nComparing {metric}:\")\n",
    "    print(f\"Unbalanced mean difference: {np.mean(real_diff):.4f}\")\n",
    "    print(f\"Balanced mean difference: {np.mean(balanced_diff):.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_difference(results, metric):\n",
    "    eth0 = results[f'{metric}_ls_eth0']\n",
    "    eth1 = results[f'{metric}_ls_eth1']\n",
    "    return [e0 - e1 for e1, e0 in zip(eth1, eth0)]\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), stats.sem(a)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "metrics_to_compare = ['Overall_ACC', 'Overall_AUC', 'Overall_MCC']\n",
    "\n",
    "for metric in metrics_to_compare:\n",
    "    real_diff = calculate_difference(results_real, metric)\n",
    "    balanced_diff = calculate_difference(results_balanced, metric)\n",
    "    \n",
    "    real_mean, real_ci_low, real_ci_high = mean_confidence_interval(real_diff)\n",
    "    balanced_mean, balanced_ci_low, balanced_ci_high = mean_confidence_interval(balanced_diff)\n",
    "    \n",
    "    print(f\"\\nComparing {metric}:\")\n",
    "    print(f\"Unbalanced mean difference: {real_mean:.4f} (95% CI: {real_ci_low:.4f} to {real_ci_high:.4f})\")\n",
    "    print(f\"Balanced mean difference: {balanced_mean:.4f} (95% CI: {balanced_ci_low:.4f} to {balanced_ci_high:.4f})\")\n",
    "    \n",
    "    if abs(balanced_mean) < abs(real_mean):\n",
    "        print(\"Balanced method reduced bias.\")\n",
    "    else:\n",
    "        print(\"Balanced method did not reduce bias.\")\n",
    "    \n",
    "    if not (real_ci_low <= balanced_mean <= real_ci_high):\n",
    "        print(\"The difference is statistically significant at the 95% confidence level.\")\n",
    "    else:\n",
    "        print(\"The difference is not statistically significant at the 95% confidence level.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_difference(results, metric):\n",
    "    eth0 = results[f'{metric}_ls_eth0']\n",
    "    eth1 = results[f'{metric}_ls_eth1']\n",
    "    \n",
    "    # Function to average every n elements\n",
    "    def average_groups(lst):\n",
    "        n = 3\n",
    "        return [sum(lst[i:i+n]) / n for i in range(0, len(lst), n)]\n",
    "    \n",
    "    # Average the results for each ethnicity\n",
    "    avg_eth0 = average_groups(eth0)\n",
    "    avg_eth1 = average_groups(eth1)\n",
    "    \n",
    "    # Calculate the difference between averaged results\n",
    "    return [e0 - e1 for e1, e0 in zip(avg_eth1, avg_eth0)]\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), stats.sem(a)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "metrics_to_compare = ['Overall_ACC', 'Overall_AUC', 'Overall_MCC']\n",
    "\n",
    "for metric in metrics_to_compare:\n",
    "    real_diff = calculate_difference(results_real, metric)\n",
    "    balanced_diff = calculate_difference(results_balanced, metric)\n",
    "    \n",
    "    real_mean, real_ci_low, real_ci_high = mean_confidence_interval(real_diff)\n",
    "    balanced_mean, balanced_ci_low, balanced_ci_high = mean_confidence_interval(balanced_diff)\n",
    "    \n",
    "    print(f\"\\nComparing {metric}:\")\n",
    "    print(f\"Unbalanced mean difference: {real_mean:.4f} (95% CI: {real_ci_low:.4f} to {real_ci_high:.4f})\")\n",
    "    print(f\"Balanced mean difference: {balanced_mean:.4f} (95% CI: {balanced_ci_low:.4f} to {balanced_ci_high:.4f})\")\n",
    "    \n",
    "    if abs(balanced_mean) < abs(real_mean):\n",
    "        print(\"Balanced method reduced bias.\")\n",
    "    else:\n",
    "        print(\"Balanced method did not reduce bias.\")\n",
    "    \n",
    "    if not (real_ci_low <= balanced_mean <= real_ci_high):\n",
    "        print(\"The difference is statistically significant at the 95% confidence level.\")\n",
    "    else:\n",
    "        print(\"The difference is not statistically significant at the 95% confidence level.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_compare = ['Overall_ACC', 'Overall_AUC', 'Overall_MCC']\n",
    "\n",
    "for metric in metrics_to_compare:\n",
    "    real_diff = calculate_difference(results_real, metric)\n",
    "    balanced_diff = calculate_difference(results_balanced, metric)\n",
    "    \n",
    "    print(f\"\\nComparing {metric}:\")\n",
    "    print(f\"Unbalanced mean difference: {np.mean(real_diff):.4f}\")\n",
    "    print(f\"Balanced mean difference: {np.mean(balanced_diff):.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_difference_full(results, metric):\n",
    "    full = results[f'{metric}_ls']\n",
    "    eth1 = results[f'{metric}_ls_eth1']\n",
    "    \n",
    "    # Function to average every n elements\n",
    "    def average_groups(lst):\n",
    "        n = 3\n",
    "        return [sum(lst[i:i+n]) / n for i in range(0, len(lst), n)]\n",
    "    \n",
    "    # Average the results \n",
    "    avg_full = average_groups(full)\n",
    "    avg_eth1 = average_groups(eth1)\n",
    "    \n",
    "    # Calculate the difference between averaged results\n",
    "    return [e0 - e1 for e1, e0 in zip(avg_eth1, avg_full)]\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), stats.sem(a)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "metrics_to_compare = ['Overall_ACC', 'Overall_AUC', 'Overall_MCC']\n",
    "\n",
    "print(\"Full dataset vs minority class \\n\")\n",
    "\n",
    "for metric in metrics_to_compare:\n",
    "    real_diff = calculate_difference_full(results_real, metric)\n",
    "    balanced_diff = calculate_difference_full(results_balanced, metric)\n",
    "    \n",
    "    real_mean, real_ci_low, real_ci_high = mean_confidence_interval(real_diff)\n",
    "    balanced_mean, balanced_ci_low, balanced_ci_high = mean_confidence_interval(balanced_diff)\n",
    "    \n",
    "    print(f\"\\nComparing {metric}:\")\n",
    "    print(f\"Unbalanced mean difference: {real_mean:.4f} (95% CI: {real_ci_low:.4f} to {real_ci_high:.4f})\")\n",
    "    print(f\"Balanced mean difference: {balanced_mean:.4f} (95% CI: {balanced_ci_low:.4f} to {balanced_ci_high:.4f})\")\n",
    "    \n",
    "    if abs(balanced_mean) < abs(real_mean):\n",
    "        print(\"Balanced method reduced bias.\")\n",
    "    else:\n",
    "        print(\"Balanced method did not reduce bias.\")\n",
    "    \n",
    "    if not (real_ci_low <= balanced_mean <= real_ci_high):\n",
    "        print(\"The difference is statistically significant at the 95% confidence level.\")\n",
    "    else:\n",
    "        print(\"The difference is not statistically significant at the 95% confidence level.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
